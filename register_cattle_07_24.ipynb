{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49dab408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "It took : 9.775161743164062e-06 s to process all frames\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pcds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 416\u001b[0m\n\u001b[1;32m    413\u001b[0m execution_time \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIt took :\u001b[39m\u001b[38;5;124m\"\u001b[39m, execution_time, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms to process all frames\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 416\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreated \u001b[39m\u001b[38;5;124m\"\u001b[39m ,\u001b[38;5;28mlen\u001b[39m(\u001b[43mpcds\u001b[49m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m point clouds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pcds' is not defined"
     ]
    }
   ],
   "source": [
    "from dt_apriltags import Detector\n",
    "import cv2\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import json\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import teaserpp_python\n",
    "from numpy.linalg import inv\n",
    "from scipy.spatial import cKDTree\n",
    "import os\n",
    "import time\n",
    "from numba import jit, prange\n",
    "\n",
    "def detect_tag(img,k,tag_size):\n",
    "    at_detector = Detector(families='tagStandard41h12',\n",
    "                       nthreads=8,\n",
    "                       quad_decimate=1.0,\n",
    "                       quad_sigma=0.8,\n",
    "                       refine_edges=1,\n",
    "                       decode_sharpening=0.25,\n",
    "                       debug=0)\n",
    "    tags  = at_detector.detect(img,True,k,tag_size)\n",
    "    #print(\"detected : \", len(tags), \" tags\")\n",
    "    best_tag = min(tags, key=lambda tag: tag.pose_err)\n",
    "    # create a 4x4 identity matrix\n",
    "    H = np.eye(4)\n",
    "\n",
    "    # set the rotation\n",
    "    H[:3, :3] = best_tag.pose_R\n",
    "\n",
    "    # set the translation\n",
    "    H[:3, 3] = best_tag.pose_t[:, 0]\n",
    "    #print(\"Done detecting tags\")\n",
    "\n",
    "    return H, best_tag.tag_id\n",
    "\n",
    "def draw_registration_result(source, target, transformation):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.paint_uniform_color([1, 0.706, 0])\n",
    "    target_temp.paint_uniform_color([0, 0.651, 0.929])\n",
    "    source_temp.transform(transformation)\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp])\n",
    "\n",
    "\n",
    "def register_two_views_teaser(A_pcd_raw,B_pcd_raw,VOXEL_SIZE):\n",
    "    \n",
    "    VISUALIZE = True\n",
    "    A_pcd = A_pcd_raw.voxel_down_sample(voxel_size=VOXEL_SIZE)\n",
    "    B_pcd = B_pcd_raw.voxel_down_sample(voxel_size=VOXEL_SIZE)\n",
    "    #if VISUALIZE:\n",
    "     #   o3d.visualization.draw_geometries([A_pcd,B_pcd]) # plot downsampled A and B \n",
    "\n",
    "    A_xyz = pcd2xyz(A_pcd) # np array of size 3 by N\n",
    "    B_xyz = pcd2xyz(B_pcd) # np array of size 3 by M\n",
    "\n",
    "    print(\"Extracting FPFH features\")\n",
    "    # extract FPFH features\n",
    "    A_feats = extract_fpfh(A_pcd,VOXEL_SIZE)\n",
    "    B_feats = extract_fpfh(B_pcd,VOXEL_SIZE)\n",
    "    print(A_feats.shape)\n",
    "    print(\"Computing FPFH correspondences\")\n",
    "    # establish correspondences by nearest neighbour search in feature space\n",
    "    corrs_A, corrs_B = find_correspondences(\n",
    "        A_feats, B_feats, mutual_filter=True)\n",
    "    A_corr = A_xyz[:,corrs_A] # np array of size 3 by num_corrs\n",
    "    B_corr = B_xyz[:,corrs_B] # np array of size 3 by num_corrs\n",
    "\n",
    "    num_corrs = A_corr.shape[1]\n",
    "    print(f'FPFH generates {num_corrs} putative correspondences.')\n",
    "\n",
    "    # visualize the point clouds together with feature correspondenc\n",
    "    # robust global registration using TEASER++\n",
    "    NOISE_BOUND = VOXEL_SIZE\n",
    "    teaser_solver = get_teaser_solver(NOISE_BOUND)\n",
    "    teaser_solver.solve(A_corr,B_corr)\n",
    "    solution = teaser_solver.getSolution()\n",
    "    R_teaser = solution.rotation\n",
    "    t_teaser = solution.translation\n",
    "    T_teaser = Rt2T(R_teaser,t_teaser)\n",
    "\n",
    "    # Visualize the registration results\n",
    "    A_pcd_T_teaser = copy.deepcopy(A_pcd).transform(T_teaser)\n",
    "    #o3d.visualization.draw_geometries([A_pcd_T_teaser,B_pcd])\n",
    "\n",
    "    # local refinement using ICP\n",
    "    icp_sol = o3d.pipelines.registration.registration_icp(\n",
    "          A_pcd, B_pcd, NOISE_BOUND, T_teaser,\n",
    "          o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "          o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=100))\n",
    "    T_icp = icp_sol.transformation\n",
    "\n",
    "    # visualize the registration after ICP refinement\n",
    "    A_pcd_T_icp = copy.deepcopy(A_pcd).transform(T_icp)\n",
    "    if VISUALIZE:\n",
    "        Acopy = copy.deepcopy(A_pcd_T_icp).paint_uniform_color([0.0,0.0,1])\n",
    "        Bcopy = copy.deepcopy(B_pcd).paint_uniform_color([1.0,0.0,0.0])\n",
    "        o3d.visualization.draw_geometries([Acopy,Bcopy])\n",
    "    tformed_A = copy.deepcopy(A_pcd_raw).transform(T_icp)\n",
    "    res = o3d.geometry.PointCloud()\n",
    "    res = tformed_A + B_pcd_raw\n",
    "    \n",
    "    return res,T_icp\n",
    "\n",
    "def pcd2xyz(pcd):\n",
    "    return np.asarray(pcd.points).T\n",
    "\n",
    "def extract_fpfh(pcd, voxel_size):\n",
    "    radius_normal = voxel_size * 2\n",
    "    pcd.estimate_normals(\n",
    "      o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=30))\n",
    "\n",
    "    radius_feature = voxel_size * 5\n",
    "    fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "      pcd, o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=100))\n",
    "    return np.array(fpfh.data).T\n",
    "\n",
    "def find_knn_cpu(feat0, feat1, knn=1, return_distance=False):\n",
    "    feat1tree = cKDTree(feat1)\n",
    "    dists, nn_inds = feat1tree.query(feat0, k=knn, workers=10)\n",
    "    if return_distance:\n",
    "        return nn_inds, dists\n",
    "    else:\n",
    "        return nn_inds\n",
    "\n",
    "def find_correspondences(feats0, feats1, mutual_filter=True):\n",
    "    nns01 = find_knn_cpu(feats0, feats1, knn=1, return_distance=False)\n",
    "    corres01_idx0 = np.arange(len(nns01))\n",
    "    corres01_idx1 = nns01\n",
    "\n",
    "    if not mutual_filter:\n",
    "        return corres01_idx0, corres01_idx1\n",
    "\n",
    "    nns10 = find_knn_cpu(feats1, feats0, knn=1, return_distance=False)\n",
    "    corres10_idx1 = np.arange(len(nns10))\n",
    "    corres10_idx0 = nns10\n",
    "\n",
    "    mutual_filter = (corres10_idx0[corres01_idx1] == corres01_idx0)\n",
    "    corres_idx0 = corres01_idx0[mutual_filter]\n",
    "    corres_idx1 = corres01_idx1[mutual_filter]\n",
    "\n",
    "    return corres_idx0, corres_idx1\n",
    "\n",
    "def get_teaser_solver(noise_bound):\n",
    "    solver_params = teaserpp_python.RobustRegistrationSolver.Params()\n",
    "    solver_params.cbar2 = 1.0\n",
    "    solver_params.noise_bound = noise_bound\n",
    "    solver_params.estimate_scaling = False\n",
    "    solver_params.inlier_selection_mode = \\\n",
    "        teaserpp_python.RobustRegistrationSolver.INLIER_SELECTION_MODE.PMC_EXACT\n",
    "    solver_params.rotation_tim_graph = \\\n",
    "        teaserpp_python.RobustRegistrationSolver.INLIER_GRAPH_FORMULATION.CHAIN\n",
    "    solver_params.rotation_estimation_algorithm = \\\n",
    "        teaserpp_python.RobustRegistrationSolver.ROTATION_ESTIMATION_ALGORITHM.GNC_TLS\n",
    "    solver_params.rotation_gnc_factor = 1.4\n",
    "    solver_params.rotation_max_iterations = 10000\n",
    "    solver_params.rotation_cost_threshold = 1e-16\n",
    "    solver = teaserpp_python.RobustRegistrationSolver(solver_params)\n",
    "    return solver\n",
    "\n",
    "def Rt2T(R,t):\n",
    "    T = np.identity(4)\n",
    "    T[:3,:3] = R\n",
    "    T[:3,3] = t\n",
    "    return T \n",
    "\n",
    "def eliminate_flying_pixels(depth_image, ws, threshold):\n",
    "   \n",
    "    # Get image size\n",
    "    height, width = depth_image.shape\n",
    "    # Create an empty array for the result\n",
    "    result = np.zeros_like(depth_image, dtype=float)\n",
    "\n",
    "    # Iterate over the entire image\n",
    "    for cy in range(height):\n",
    "        for cx in range(width):\n",
    "            # Set the range for the window\n",
    "            x_start, x_end = max(0, cx - ws), min(width, cx + ws + 1)\n",
    "            y_start, y_end = max(0, cy - ws), min(height, cy + ws + 1)\n",
    "            \n",
    "            # Get the window\n",
    "            window = depth_image[y_start:y_end, x_start:x_end]\n",
    "\n",
    "            # Calculate the sum of absolute differences\n",
    "            result[cy, cx] = np.sum(np.abs(window - depth_image[cy, cx]))\n",
    "    count = np.sum(result > threshold)\n",
    "\n",
    "    depth_image[result > threshold] = 0\n",
    "    return  depth_image\n",
    "\n",
    "\n",
    "\n",
    "def colored_ICP(source, target):\n",
    "    \n",
    "    voxel_radius = [0.04, 0.02, 0.01]\n",
    "    max_iter = [50, 30, 14]\n",
    "    current_transformation = np.identity(4)\n",
    "    print(\"3. Colored point cloud registration\")\n",
    "    for scale in range(3):\n",
    "        iters = max_iter[scale]\n",
    "        radius = voxel_radius[scale]\n",
    "        print(\"iteration: \", iters, radius, scale)\n",
    "\n",
    "        print(\"3-1. Downsample with a voxel size %.2f\" % radius)\n",
    "        source_down = copy.deepcopy(source).voxel_down_sample(radius)\n",
    "        target_down = copy.deepcopy(target).voxel_down_sample(radius)\n",
    "\n",
    "        print(\"3-2. Estimate normal.\")\n",
    "        source_down.estimate_normals(\n",
    "            o3d.geometry.KDTreeSearchParamHybrid(radius=radius * 2, max_nn=30))\n",
    "        target_down.estimate_normals(\n",
    "            o3d.geometry.KDTreeSearchParamHybrid(radius=radius * 2, max_nn=30))\n",
    "\n",
    "        print(\"3-3. Applying colored point cloud registration\")\n",
    "        result_icp = o3d.pipelines.registration.registration_colored_icp(\n",
    "            source_down, target_down, radius, current_transformation,\n",
    "            o3d.pipelines.registration.TransformationEstimationForColoredICP(),\n",
    "            o3d.pipelines.registration.ICPConvergenceCriteria(relative_fitness=1e-6,\n",
    "                                                              relative_rmse=1e-6,\n",
    "                                                              max_iteration=iters))\n",
    "        current_transformation = result_icp.transformation\n",
    "    \n",
    "   \n",
    "        draw_registration_result(source, target, current_transformation)\n",
    "    return current_transformation\n",
    "\n",
    "def backproject_o3d(rgbd_frame, intrinsics):\n",
    "    \n",
    "    rgbdc = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "        rgbd_frame.color, rgbd_frame.depth, depth_trunc=4.0, convert_rgb_to_intensity=False)\n",
    "\n",
    "\n",
    "    # Create a point cloud from the RGBD image\n",
    "    pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbdc, intrinsics)\n",
    "    n_radius = 0.01*2.0\n",
    "    pcd.estimate_normals(\n",
    "    search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=n_radius, max_nn=30))\n",
    "    # Visualize the point cloud\n",
    "    #o3d.visualization.draw_geometries([pcd])\n",
    "    \n",
    "    return pcd\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def numba_eliminate_flying_pixels(depth_image, ws, threshold):\n",
    "    height, width = depth_image.shape\n",
    "    result = np.zeros_like(depth_image, dtype=np.float64)\n",
    "    \n",
    "    for cy in prange(height):\n",
    "        for cx in prange(width):\n",
    "            x_start, x_end = max(0, cx - ws), min(width, cx + ws + 1)\n",
    "            y_start, y_end = max(0, cy - ws), min(height, cy + ws + 1)\n",
    "            window = depth_image[y_start:y_end, x_start:x_end]\n",
    "            result[cy, cx] = np.sum(np.abs(window - depth_image[cy, cx]))\n",
    "    \n",
    "    #for i in prange(height):\n",
    "       # for j in prange(width):\n",
    "      #      if result[i, j] > threshold:\n",
    "     #           depth_image[i, j] = 0\n",
    "    #count = np.sum(result > threshold)\n",
    "    #print(\"Numba detected: #\", count)\n",
    "    return result\n",
    "\n",
    "\n",
    "def load_filter_pcds(data_path,flying_pixel_filter_threshold):  # returns a list of  pcds\n",
    "\n",
    "\n",
    "    l = 0.3048  # replace with the actual value\n",
    "    ws = 2\n",
    "   \n",
    "    # define a dictionary where the keys are the tag_ids and the values are the transformation matrices\n",
    "    transformations = {\n",
    "        0: np.array([[0, 1, 0, 0],\n",
    "                    [1, 0, 0, 0],\n",
    "                    [0, 0, -1, l/2],\n",
    "                    [0, 0, 0, 1]]),\n",
    "        1: np.array([[0, 0, -1, l/2],\n",
    "                    [1, 0, 0, 0],\n",
    "                    [0, -1, 0, 0],\n",
    "                    [0, 0, 0, 1]]),\n",
    "        2: np.array([[-1, 0, 0, 0],\n",
    "                    [0, 0, -1, l/2],\n",
    "                    [0, -1, 0, 0],\n",
    "                    [0, 0, 0, 1]]),\n",
    "        3: np.array([[0, 0, 1, -l/2],\n",
    "                    [-1, 0, 0, 0],\n",
    "                    [0, -1, 0, 0],\n",
    "                    [0, 0, 0, 1]]),\n",
    "        4: np.array([[1, 0, 0, 0],\n",
    "                    [0, 0, 1, -l/2],\n",
    "                    [0, -1, 0, 0],\n",
    "                    [0, 0, 0, 1]])\n",
    "    }\n",
    "    \n",
    "    tag_size = 0.145    # assuming best_tag is defined and contains the tag with the smallest pose_err\n",
    "\n",
    "    reader = o3d.io.AzureKinectMKVReader()\n",
    "    abspath = data_path\n",
    "    \n",
    "        \n",
    "    files = glob.glob(data_path+'/*.mkv')\n",
    "    files.sort()\n",
    "    #print(\"There are : \", len(files), \"present here!\")\n",
    "\n",
    "    list_size = len(files)\n",
    "    rgbd_frames = [None] * list_size\n",
    "\n",
    "    tag_poses = [None] * list_size  # assuming tag_poses and current_transform are dictionaries\n",
    "    current_transform = [None] * list_size\n",
    "    pcds = []\n",
    "    for i in range(len(files)): # for each view\n",
    "        inFile = files[i]\n",
    "        fname = inFile.split('/')[-1]\n",
    "        file_name = fname.split('.mkv')[0]\n",
    "        #print(\"Current File: \", file_name)\n",
    "\n",
    "        reader.open(inFile)\n",
    "        if not reader.is_opened():\n",
    "            raise RuntimeError(\"Unable to open file {}\".format(inFile))\n",
    "        metadata = reader.get_metadata()\n",
    "  \n",
    "        # write the metadata to a JSON file since that seems to be the only\n",
    "        # way to retrieve that data\n",
    "        o3d.io.write_azure_kinect_mkv_metadata(\n",
    "                    '{}/{}_intrinsic.json'.format(abspath,file_name), metadata)\n",
    "\n",
    "        # Open the file and load the JSON\n",
    "        with open(abspath+\"/\" + file_name + \"_intrinsic.json\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        height = data['height']\n",
    "        width = data['width']\n",
    "        intrinsics = data[\"intrinsic_matrix\"]\n",
    "        camera_intrinsics = o3d.camera.PinholeCameraIntrinsic()\n",
    "        cx = intrinsics[6]\n",
    "        cy = intrinsics[7]\n",
    "        fx = intrinsics[0]\n",
    "        fy = intrinsics[4]\n",
    "        camera_intrinsics.set_intrinsics(width,height,fx,fy,cx,cy)\n",
    "        K = (fx,fy,cx,cy)\n",
    "\n",
    "        last_frame = None\n",
    "        while not reader.is_eof(): # go until hitting eof because of exposure issues in early color frames\n",
    "            rgbda = reader.next_frame()\n",
    "            if rgbda is None:\n",
    "                #print(\"Got nothing! \")\n",
    "                continue\n",
    "            last_frame = rgbda\n",
    "\n",
    "        if last_frame is not None:\n",
    "            #print(\"Got the last frame\")\n",
    "            rgbd_frames[i] = last_frame\n",
    "        else:\n",
    "            \n",
    "            print(\"************No valid frames found in the .mkv file.**********\")\n",
    "\n",
    "        rgb_im_np = np.asarray(last_frame.color)\n",
    "    \n",
    "        gray_img = cv2.cvtColor(rgb_im_np, cv2.COLOR_RGB2GRAY)\n",
    "        im_name = \"img_\" + str(i) + \".jpg\" \n",
    "        cv2.imwrite(im_name, gray_img)\n",
    "        transform, tag_id = detect_tag(gray_img,K, tag_size)\n",
    "        #print(\"T: \", transform)\n",
    "        #print(\"ID: \", tag_id)\n",
    "        \n",
    "        # get the transformation matrix for the given tag_id\n",
    "        cube_transform = transformations.get(tag_id, np.eye(4))  # defaults to identity matrix if tag_id is not found\n",
    "\n",
    "        tag_poses[i] = np.dot(transform, np.linalg.inv(cube_transform))\n",
    "        \n",
    "        # assuming tag_poses[0] is defined and is the pose of the first camera\n",
    "        tag_pose = np.dot(tag_poses[0], np.linalg.inv(tag_poses[i]))\n",
    "\n",
    "        current_transform[i] = tag_pose  # current_transform is now the tag pose\n",
    "        reader.close()\n",
    "        fname_tform = \"H_0_\" + str(i) + \".txt\"\n",
    "        np.savetxt(data_path+fname_tform, current_transform[i])\n",
    "        depth_image_array = np.asarray(last_frame.depth) # reference the same depth so it will change\n",
    "         \n",
    "        start_time_numba = time.time()\n",
    "        result_mask = numba_eliminate_flying_pixels(depth_image_array.copy(), ws, flying_pixel_filter_threshold)\n",
    "        end_time_numba = time.time()\n",
    "\n",
    "        # apply thresholding\n",
    "        #count = np.sum(result_mask > flying_pixel_filter_threshold)\n",
    "        #print(\"Numba Detected :#\", count)\n",
    "\n",
    "        depth_image_array[result_mask > flying_pixel_filter_threshold] = 0\n",
    "        filtered_depth = depth_image_array\n",
    "        execution_time_numba = end_time_numba - start_time_numba\n",
    "        #filtered_depth = bilateral_filter(depth_image_array)\n",
    "        last_frame.depth = o3d.geometry.Image(filtered_depth)\n",
    "        pcd = backproject_o3d(last_frame, camera_intrinsics)\n",
    "        #o3d.visualization.draw_geometries([pcd])\n",
    "        #break\n",
    "        pcds.append(copy.deepcopy(pcd).transform(current_transform[i]))\n",
    "\n",
    "    #o3d.visualization.draw_geometries(pcds)\n",
    "    return pcds,current_transform\n",
    "\n",
    "\n",
    "def compute_and_orient_normals(pcd, voxel_size):\n",
    "    normal_radius = voxel_size*2.0\n",
    "    pcd.estimate_normals(\n",
    "    search_param=o3d.geometry.KDTreeSearchParamHybrid(normal_radius=0.1, max_nn=30))\n",
    "    \n",
    "data_path = '/home/vigir3d/Datasets/cattle_scans/farm_07_28/Animal_calib_new4/'\n",
    "o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Error)\n",
    "start_time = time.time()\n",
    "#pcds, tforms = load_filter_pcds(data_path,1200) # lower threshold will remove more points()\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(\"It took :\", execution_time, \"s to process all frames\")\n",
    "\n",
    "print(\"Created \" ,len(pcds), \" point clouds\")\n",
    "\n",
    "# t01= colored_ICP(pcds[0],pcds[1])\n",
    "# t52= colored_ICP(pcds[5],pcds[2])\n",
    "# t43= colored_ICP(pcds[4],pcds[3])\n",
    "# t12= colored_ICP(pcds[1],pcds[2])\n",
    "# t32= colored_ICP(pcds[3],pcds[2])\n",
    "\n",
    "\n",
    "# H0 = t12 @ t01\n",
    "# H1 = t12\n",
    "# H3 = t32\n",
    "# H4 = t32 @ t43\n",
    "# H5 = t52\n",
    "\n",
    "\n",
    "# np.savetxt(\"c_icp_0_2.txt\", H0)\n",
    "# np.savetxt(\"c_icp_1_2.txt\", H1)\n",
    "# np.savetxt(\"c_icp_3_2.txt\", H3)\n",
    "# np.savetxt(\"c_icp_4_2.txt\", H4)\n",
    "# np.savetxt(\"c_icp_5_2.txt\", H5)\n",
    "\n",
    "# p0 = copy.deepcopy(pcds[0]).transform(H0)\n",
    "# p1 = copy.deepcopy(pcds[1]).transform(H1)\n",
    "# p3 = copy.deepcopy(pcds[3]).transform(H3)\n",
    "# p4 = copy.deepcopy(pcds[4]).transform(H4)\n",
    "# p5 = copy.deepcopy(pcds[5]).transform(H5)\n",
    "\n",
    "# o3d.visualization.draw_geometries([p0,p1,pcds[2],p3,p4,p5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8222852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transformations(tform_path):\n",
    "    \"\"\"Load transformations from the given path.\"\"\"\n",
    "    htm_files = [\"htm_0_3.txt\", \"htm_1_3.txt\", \"htm_5_3.txt\", \"htm_6_3.txt\", \"htm_7_3.txt\"]\n",
    "    return [np.loadtxt(tform_path + file) for file in htm_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17568ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tform_path = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062b057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dt_apriltags import Detector\n",
    "import cv2\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import json\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import teaserpp_python\n",
    "from numpy.linalg import inv\n",
    "from scipy.spatial import cKDTree\n",
    "import os\n",
    "import time\n",
    "from numba import jit, prange\n",
    "import argparse\n",
    "\n",
    "def detect_tag(img,k,tag_size):\n",
    "    at_detector = Detector(families='tagStandard41h12',\n",
    "                       nthreads=8,\n",
    "                       quad_decimate=1.0,\n",
    "                       quad_sigma=0.8,\n",
    "                       refine_edges=1,\n",
    "                       decode_sharpening=0.25,\n",
    "                       debug=0)\n",
    "    tags  = at_detector.detect(img,True,k,tag_size)\n",
    "    #print(\"detected : \", len(tags), \" tags\")\n",
    "    best_tag = min(tags, key=lambda tag: tag.pose_err)\n",
    "    # create a 4x4 identity matrix\n",
    "    H = np.eye(4)\n",
    "\n",
    "    # set the rotation\n",
    "    H[:3, :3] = best_tag.pose_R\n",
    "\n",
    "    # set the translation\n",
    "    H[:3, 3] = best_tag.pose_t[:, 0]\n",
    "    #print(\"Done detecting tags\")\n",
    "\n",
    "    return H, best_tag.tag_id\n",
    "\n",
    "def draw_registration_result(source, target, transformation):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.paint_uniform_color([1, 0.706, 0])\n",
    "    target_temp.paint_uniform_color([0, 0.651, 0.929])\n",
    "    source_temp.transform(transformation)\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp])\n",
    "\n",
    "\n",
    "def register_two_views_teaser(A_pcd_raw,B_pcd_raw,VOXEL_SIZE):\n",
    "    \n",
    "    VISUALIZE = True\n",
    "    A_pcd = A_pcd_raw.voxel_down_sample(voxel_size=VOXEL_SIZE)\n",
    "    B_pcd = B_pcd_raw.voxel_down_sample(voxel_size=VOXEL_SIZE)\n",
    "    #if VISUALIZE:\n",
    "     #   o3d.visualization.draw_geometries([A_pcd,B_pcd]) # plot downsampled A and B \n",
    "\n",
    "    A_xyz = pcd2xyz(A_pcd) # np array of size 3 by N\n",
    "    B_xyz = pcd2xyz(B_pcd) # np array of size 3 by M\n",
    "\n",
    "    print(\"Extracting FPFH features\")\n",
    "    # extract FPFH features\n",
    "    A_feats = extract_fpfh(A_pcd,VOXEL_SIZE)\n",
    "    B_feats = extract_fpfh(B_pcd,VOXEL_SIZE)\n",
    "    print(A_feats.shape)\n",
    "    print(\"Computing FPFH correspondences\")\n",
    "    # establish correspondences by nearest neighbour search in feature space\n",
    "    corrs_A, corrs_B = find_correspondences(\n",
    "        A_feats, B_feats, mutual_filter=True)\n",
    "    A_corr = A_xyz[:,corrs_A] # np array of size 3 by num_corrs\n",
    "    B_corr = B_xyz[:,corrs_B] # np array of size 3 by num_corrs\n",
    "\n",
    "    num_corrs = A_corr.shape[1]\n",
    "    print(f'FPFH generates {num_corrs} putative correspondences.')\n",
    "\n",
    "    # visualize the point clouds together with feature correspondenc\n",
    "    # robust global registration using TEASER++\n",
    "    NOISE_BOUND = VOXEL_SIZE\n",
    "    teaser_solver = get_teaser_solver(NOISE_BOUND)\n",
    "    teaser_solver.solve(A_corr,B_corr)\n",
    "    solution = teaser_solver.getSolution()\n",
    "    R_teaser = solution.rotation\n",
    "    t_teaser = solution.translation\n",
    "    T_teaser = Rt2T(R_teaser,t_teaser)\n",
    "\n",
    "    # Visualize the registration results\n",
    "    A_pcd_T_teaser = copy.deepcopy(A_pcd).transform(T_teaser)\n",
    "    #o3d.visualization.draw_geometries([A_pcd_T_teaser,B_pcd])\n",
    "\n",
    "    # local refinement using ICP\n",
    "    icp_sol = o3d.pipelines.registration.registration_icp(\n",
    "          A_pcd, B_pcd, NOISE_BOUND, T_teaser,\n",
    "          o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "          o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=100))\n",
    "    T_icp = icp_sol.transformation\n",
    "\n",
    "    # visualize the registration after ICP refinement\n",
    "    A_pcd_T_icp = copy.deepcopy(A_pcd).transform(T_icp)\n",
    "    if VISUALIZE:\n",
    "        Acopy = copy.deepcopy(A_pcd_T_icp).paint_uniform_color([0.0,0.0,1])\n",
    "        Bcopy = copy.deepcopy(B_pcd).paint_uniform_color([1.0,0.0,0.0])\n",
    "        o3d.visualization.draw_geometries([Acopy,Bcopy])\n",
    "    tformed_A = copy.deepcopy(A_pcd_raw).transform(T_icp)\n",
    "    res = o3d.geometry.PointCloud()\n",
    "    res = tformed_A + B_pcd_raw\n",
    "    \n",
    "    return res,T_icp\n",
    "\n",
    "def pcd2xyz(pcd):\n",
    "    return np.asarray(pcd.points).T\n",
    "\n",
    "def extract_fpfh(pcd, voxel_size):\n",
    "    radius_normal = voxel_size * 2\n",
    "    pcd.estimate_normals(\n",
    "      o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=30))\n",
    "\n",
    "    radius_feature = voxel_size * 5\n",
    "    fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "      pcd, o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=100))\n",
    "    return np.array(fpfh.data).T\n",
    "\n",
    "def find_knn_cpu(feat0, feat1, knn=1, return_distance=False):\n",
    "    feat1tree = cKDTree(feat1)\n",
    "    dists, nn_inds = feat1tree.query(feat0, k=knn, workers=10)\n",
    "    if return_distance:\n",
    "        return nn_inds, dists\n",
    "    else:\n",
    "        return nn_inds\n",
    "\n",
    "def find_correspondences(feats0, feats1, mutual_filter=True):\n",
    "    nns01 = find_knn_cpu(feats0, feats1, knn=1, return_distance=False)\n",
    "    corres01_idx0 = np.arange(len(nns01))\n",
    "    corres01_idx1 = nns01\n",
    "\n",
    "    if not mutual_filter:\n",
    "        return corres01_idx0, corres01_idx1\n",
    "\n",
    "    nns10 = find_knn_cpu(feats1, feats0, knn=1, return_distance=False)\n",
    "    corres10_idx1 = np.arange(len(nns10))\n",
    "    corres10_idx0 = nns10\n",
    "\n",
    "    mutual_filter = (corres10_idx0[corres01_idx1] == corres01_idx0)\n",
    "    corres_idx0 = corres01_idx0[mutual_filter]\n",
    "    corres_idx1 = corres01_idx1[mutual_filter]\n",
    "\n",
    "    return corres_idx0, corres_idx1\n",
    "\n",
    "def get_teaser_solver(noise_bound):\n",
    "    solver_params = teaserpp_python.RobustRegistrationSolver.Params()\n",
    "    solver_params.cbar2 = 1.0\n",
    "    solver_params.noise_bound = noise_bound\n",
    "    solver_params.estimate_scaling = False\n",
    "    solver_params.inlier_selection_mode = \\\n",
    "        teaserpp_python.RobustRegistrationSolver.INLIER_SELECTION_MODE.PMC_EXACT\n",
    "    solver_params.rotation_tim_graph = \\\n",
    "        teaserpp_python.RobustRegistrationSolver.INLIER_GRAPH_FORMULATION.CHAIN\n",
    "    solver_params.rotation_estimation_algorithm = \\\n",
    "        teaserpp_python.RobustRegistrationSolver.ROTATION_ESTIMATION_ALGORITHM.GNC_TLS\n",
    "    solver_params.rotation_gnc_factor = 1.4\n",
    "    solver_params.rotation_max_iterations = 10000\n",
    "    solver_params.rotation_cost_threshold = 1e-16\n",
    "    solver = teaserpp_python.RobustRegistrationSolver(solver_params)\n",
    "    return solver\n",
    "\n",
    "def Rt2T(R,t):\n",
    "    T = np.identity(4)\n",
    "    T[:3,:3] = R\n",
    "    T[:3,3] = t\n",
    "    return T \n",
    "\n",
    "def eliminate_flying_pixels(depth_image, ws, threshold):\n",
    "   \n",
    "    # Get image size\n",
    "    height, width = depth_image.shape\n",
    "    # Create an empty array for the result\n",
    "    result = np.zeros_like(depth_image, dtype=float)\n",
    "\n",
    "    # Iterate over the entire image\n",
    "    for cy in range(height):\n",
    "        for cx in range(width):\n",
    "            # Set the range for the window\n",
    "            x_start, x_end = max(0, cx - ws), min(width, cx + ws + 1)\n",
    "            y_start, y_end = max(0, cy - ws), min(height, cy + ws + 1)\n",
    "            \n",
    "            # Get the window\n",
    "            window = depth_image[y_start:y_end, x_start:x_end]\n",
    "\n",
    "            # Calculate the sum of absolute differences\n",
    "            result[cy, cx] = np.sum(np.abs(window - depth_image[cy, cx]))\n",
    "    count = np.sum(result > threshold)\n",
    "\n",
    "    depth_image[result > threshold] = 0\n",
    "    return  depth_image\n",
    "\n",
    "\n",
    "\n",
    "def colored_ICP(source, target):\n",
    "    \n",
    "    voxel_radius = [0.04, 0.02, 0.01]\n",
    "    max_iter = [50, 30, 14]\n",
    "    current_transformation = np.identity(4)\n",
    "    print(\"3. Colored point cloud registration\")\n",
    "    for scale in range(3):\n",
    "        iters = max_iter[scale]\n",
    "        radius = voxel_radius[scale]\n",
    "        print(\"iteration: \", iters, radius, scale)\n",
    "\n",
    "        print(\"3-1. Downsample with a voxel size %.2f\" % radius)\n",
    "        source_down = copy.deepcopy(source).voxel_down_sample(radius)\n",
    "        target_down = copy.deepcopy(target).voxel_down_sample(radius)\n",
    "\n",
    "        print(\"3-2. Estimate normal.\")\n",
    "        source_down.estimate_normals(\n",
    "            o3d.geometry.KDTreeSearchParamHybrid(radius=radius * 2, max_nn=30))\n",
    "        target_down.estimate_normals(\n",
    "            o3d.geometry.KDTreeSearchParamHybrid(radius=radius * 2, max_nn=30))\n",
    "\n",
    "        print(\"3-3. Applying colored point cloud registration\")\n",
    "        result_icp = o3d.pipelines.registration.registration_colored_icp(\n",
    "            source_down, target_down, radius, current_transformation,\n",
    "            o3d.pipelines.registration.TransformationEstimationForColoredICP(),\n",
    "            o3d.pipelines.registration.ICPConvergenceCriteria(relative_fitness=1e-6,\n",
    "                                                              relative_rmse=1e-6,\n",
    "                                                              max_iteration=iters))\n",
    "        current_transformation = result_icp.transformation\n",
    "    \n",
    "   \n",
    "        draw_registration_result(source, target, current_transformation)\n",
    "    return current_transformation\n",
    "\n",
    "def backproject_o3d(rgbd_frame, intrinsics):\n",
    "    \n",
    "    rgbdc = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "        rgbd_frame.color, rgbd_frame.depth, depth_trunc=4.0, convert_rgb_to_intensity=False)\n",
    "\n",
    "\n",
    "    # Create a point cloud from the RGBD image\n",
    "    pcd = o3d.geometry.PointCloud.create_from_rgbd_image(rgbdc, intrinsics)\n",
    "    n_radius = 0.01*2.0\n",
    "    pcd.estimate_normals(\n",
    "    search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=n_radius, max_nn=30))\n",
    "    # Visualize the point cloud\n",
    "    #o3d.visualization.draw_geometries([pcd])\n",
    "    \n",
    "    return pcd\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def numba_eliminate_flying_pixels(depth_image, ws, threshold):\n",
    "    height, width = depth_image.shape\n",
    "    result = np.zeros_like(depth_image, dtype=np.float64)\n",
    "    \n",
    "    for cy in prange(height):\n",
    "        for cx in prange(width):\n",
    "            x_start, x_end = max(0, cx - ws), min(width, cx + ws + 1)\n",
    "            y_start, y_end = max(0, cy - ws), min(height, cy + ws + 1)\n",
    "            window = depth_image[y_start:y_end, x_start:x_end]\n",
    "            result[cy, cx] = np.sum(np.abs(window - depth_image[cy, cx]))\n",
    "    \n",
    "    #for i in prange(height):\n",
    "       # for j in prange(width):\n",
    "      #      if result[i, j] > threshold:\n",
    "     #           depth_image[i, j] = 0\n",
    "    #count = np.sum(result > threshold)\n",
    "    #print(\"Numba detected: #\", count)\n",
    "    return result\n",
    "\n",
    "\n",
    "def load_filter_pcds(data_path,flying_pixel_filter_threshold):  # returns a list of  pcds\n",
    "\n",
    "\n",
    "    l = 0.3048  # replace with the actual value\n",
    "    ws = 2\n",
    "   \n",
    "    # define a dictionary where the keys are the tag_ids and the values are the transformation matrices\n",
    "    transformations = {\n",
    "        0: np.array([[0, 1, 0, 0],\n",
    "                    [1, 0, 0, 0],\n",
    "                    [0, 0, -1, l/2],\n",
    "                    [0, 0, 0, 1]]),\n",
    "        1: np.array([[0, 0, -1, l/2],\n",
    "                    [1, 0, 0, 0],\n",
    "                    [0, -1, 0, 0],\n",
    "                    [0, 0, 0, 1]]),\n",
    "        2: np.array([[-1, 0, 0, 0],\n",
    "                    [0, 0, -1, l/2],\n",
    "                    [0, -1, 0, 0],\n",
    "                    [0, 0, 0, 1]]),\n",
    "        3: np.array([[0, 0, 1, -l/2],\n",
    "                    [-1, 0, 0, 0],\n",
    "                    [0, -1, 0, 0],\n",
    "                    [0, 0, 0, 1]]),\n",
    "        4: np.array([[1, 0, 0, 0],\n",
    "                    [0, 0, 1, -l/2],\n",
    "                    [0, -1, 0, 0],\n",
    "                    [0, 0, 0, 1]])\n",
    "    }\n",
    "    \n",
    "    tag_size = 0.145    # assuming best_tag is defined and contains the tag with the smallest pose_err\n",
    "\n",
    "    reader = o3d.io.AzureKinectMKVReader()\n",
    "    abspath = data_path\n",
    "    \n",
    "        \n",
    "    files = glob.glob(data_path+'/*.mkv')\n",
    "    files.sort()\n",
    "    #print(\"There are : \", len(files), \"present here!\")\n",
    "\n",
    "    list_size = len(files)\n",
    "    rgbd_frames = [None] * list_size\n",
    "\n",
    "    tag_poses = [None] * list_size  # assuming tag_poses and current_transform are dictionaries\n",
    "    current_transform = [None] * list_size\n",
    "    pcds = []\n",
    "    for i in range(len(files)): # for each view\n",
    "        inFile = files[i]\n",
    "        fname = inFile.split('/')[-1]\n",
    "        file_name = fname.split('.mkv')[0]\n",
    "        #print(\"Current File: \", file_name)\n",
    "\n",
    "        reader.open(inFile)\n",
    "        if not reader.is_opened():\n",
    "            raise RuntimeError(\"Unable to open file {}\".format(inFile))\n",
    "        metadata = reader.get_metadata()\n",
    "  \n",
    "        # write the metadata to a JSON file since that seems to be the only\n",
    "        # way to retrieve that data\n",
    "        o3d.io.write_azure_kinect_mkv_metadata(\n",
    "                    '{}/{}_intrinsic.json'.format(abspath,file_name), metadata)\n",
    "\n",
    "        # Open the file and load the JSON\n",
    "        with open(abspath+\"/\" + file_name + \"_intrinsic.json\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        height = data['height']\n",
    "        width = data['width']\n",
    "        intrinsics = data[\"intrinsic_matrix\"]\n",
    "        camera_intrinsics = o3d.camera.PinholeCameraIntrinsic()\n",
    "        cx = intrinsics[6]\n",
    "        cy = intrinsics[7]\n",
    "        fx = intrinsics[0]\n",
    "        fy = intrinsics[4]\n",
    "        camera_intrinsics.set_intrinsics(width,height,fx,fy,cx,cy)\n",
    "        K = (fx,fy,cx,cy)\n",
    "\n",
    "        last_frame = None\n",
    "        while not reader.is_eof(): # go until hitting eof because of exposure issues in early color frames\n",
    "            rgbda = reader.next_frame()\n",
    "            if rgbda is None:\n",
    "                #print(\"Got nothing! \")\n",
    "                continue\n",
    "            last_frame = rgbda\n",
    "\n",
    "        if last_frame is not None:\n",
    "            #print(\"Got the last frame\")\n",
    "            rgbd_frames[i] = last_frame\n",
    "        else:\n",
    "            \n",
    "            print(\"************No valid frames found in the .mkv file.**********\")\n",
    "\n",
    "        rgb_im_np = np.asarray(last_frame.color)\n",
    "    \n",
    "        gray_img = cv2.cvtColor(rgb_im_np, cv2.COLOR_RGB2GRAY)\n",
    "        im_name = \"img_\" + str(i) + \".jpg\" \n",
    "        cv2.imwrite(im_name, gray_img)\n",
    "        transform, tag_id = detect_tag(gray_img,K, tag_size)\n",
    "        #print(\"T: \", transform)\n",
    "        #print(\"ID: \", tag_id)\n",
    "        \n",
    "        # get the transformation matrix for the given tag_id\n",
    "        cube_transform = transformations.get(tag_id, np.eye(4))  # defaults to identity matrix if tag_id is not found\n",
    "\n",
    "        tag_poses[i] = np.dot(transform, np.linalg.inv(cube_transform))\n",
    "        \n",
    "        # assuming tag_poses[0] is defined and is the pose of the first camera\n",
    "        tag_pose = np.dot(tag_poses[0], np.linalg.inv(tag_poses[i]))\n",
    "\n",
    "        current_transform[i] = tag_pose  # current_transform is now the tag pose\n",
    "        reader.close()\n",
    "        # WRITE THE TRANSFORMATIONS\n",
    "        fname_tform = \"H_0_\" + str(i) + \".txt\"\n",
    "        np.savetxt(data_path+fname_tform, current_transform[i])\n",
    "        depth_image_array = np.asarray(last_frame.depth) # reference the same depth so it will change\n",
    "         \n",
    "        #start_time_numba = time.time()\n",
    "        result_mask = numba_eliminate_flying_pixels(depth_image_array.copy(), ws, flying_pixel_filter_threshold)\n",
    "        #end_time_numba = time.time()\n",
    "\n",
    "        # apply thresholding\n",
    "        #count = np.sum(result_mask > flying_pixel_filter_threshold)\n",
    "        #print(\"Numba Detected :#\", count)\n",
    "\n",
    "        depth_image_array[result_mask > flying_pixel_filter_threshold] = 0\n",
    "        filtered_depth = depth_image_array\n",
    "        #execution_time_numba = end_time_numba - start_time_numba\n",
    "        #filtered_depth = bilateral_filter(depth_image_array)\n",
    "        last_frame.depth = o3d.geometry.Image(filtered_depth)\n",
    "        pcd = backproject_o3d(last_frame, camera_intrinsics)\n",
    "        #o3d.visualization.draw_geometries([pcd])\n",
    "        #break\n",
    "        pcds.append(copy.deepcopy(pcd).transform(current_transform[i]))\n",
    "\n",
    "    o3d.visualization.draw_geometries(pcds)\n",
    "    return pcds,current_transform\n",
    "\n",
    "\n",
    "def compute_and_orient_normals(pcd, voxel_size):\n",
    "    normal_radius = voxel_size*2.0\n",
    "    pcd.estimate_normals(\n",
    "    search_param=o3d.geometry.KDTreeSearchParamHybrid(normal_radius=0.1, max_nn=30))\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #parser = argparse.ArgumentParser(description=\"Perform Azure Kinect calibration using the apriltag cube.\")\n",
    "    #parser.add_argument(\"-i\", \"--input\", required=True, help=\"Path to the calibration data.\")\n",
    "    #args = parser.parse_args()\n",
    "    \n",
    "    #main(args.input, args.transform)\n",
    "    data_path = '/home/vigir3d/Datasets/cattle_scans/farm_07_28/Animal_calib_new4/'\n",
    "    o3d.utility.set_verbosity_level(o3d.utility.VerbosityLevel.Error)\n",
    "    start_time = time.time()\n",
    "    pcds, tforms = load_filter_pcds(data_path,1200) # lower threshold will remove more points()\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(\"It took :\", execution_time, \"s to process all frames\")\n",
    "\n",
    "    print(\"Created \" ,len(pcds), \" point clouds\")\n",
    "    #o3d.visualization.draw_geometries(pcds)\n",
    "\n",
    "# t01= colored_ICP(pcds[0],pcds[1])\n",
    "# t52= colored_ICP(pcds[5],pcds[2])\n",
    "# t43= colored_ICP(pcds[4],pcds[3])\n",
    "# t12= colored_ICP(pcds[1],pcds[2])\n",
    "# t32= colored_ICP(pcds[3],pcds[2])\n",
    "\n",
    "\n",
    "# H0 = t12 @ t01\n",
    "# H1 = t12\n",
    "# H3 = t32\n",
    "# H4 = t32 @ t43\n",
    "# H5 = t52\n",
    "\n",
    "\n",
    "# np.savetxt(\"c_icp_0_2.txt\", H0)\n",
    "# np.savetxt(\"c_icp_1_2.txt\", H1)\n",
    "# np.savetxt(\"c_icp_3_2.txt\", H3)\n",
    "# np.savetxt(\"c_icp_4_2.txt\", H4)\n",
    "# np.savetxt(\"c_icp_5_2.txt\", H5)\n",
    "\n",
    "# p0 = copy.deepcopy(pcds[0]).transform(H0)\n",
    "# p1 = copy.deepcopy(pcds[1]).transform(H1)\n",
    "# p3 = copy.deepcopy(pcds[3]).transform(H3)\n",
    "# p4 = copy.deepcopy(pcds[4]).transform(H4)\n",
    "# p5 = copy.deepcopy(pcds[5]).transform(H5)\n",
    "\n",
    "# o3d.visualization.draw_geometries([p0,p1,pcds[2],p3,p4,p5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b08c999b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import threading\n",
    "\n",
    "def eliminate_flying_pixels_worker(depth_image, ws, threshold, y_start, y_end, result):\n",
    "    \"\"\"Worker function to process a chunk of the image.\"\"\"\n",
    "    \n",
    "    height, width = depth_image.shape\n",
    "    \n",
    "    # Iterate over the chunk of the image assigned to this thread\n",
    "    for cy in range(y_start, y_end):\n",
    "        for cx in range(width):\n",
    "            # Set the range for the window\n",
    "            x_start, x_end = max(0, cx - ws), min(width, cx + ws + 1)\n",
    "            \n",
    "            # Get the window\n",
    "            window = depth_image[max(0, cy - ws):min(height, cy + ws + 1), x_start:x_end]\n",
    "\n",
    "            # Calculate the sum of absolute differences\n",
    "            result[cy, cx] = np.sum(np.abs(window - depth_image[cy, cx]))\n",
    "\n",
    "def parallel_eliminate_flying_pixels(depth_image, ws, threshold, num_threads=4):\n",
    "    height, width = depth_image.shape\n",
    "    result = np.zeros_like(depth_image, dtype=float)\n",
    "    \n",
    "    # Create threads and split the image into chunks for each thread\n",
    "    threads = []\n",
    "    chunk_size = height // num_threads\n",
    "    for i in range(num_threads):\n",
    "        y_start = i * chunk_size\n",
    "        y_end = (i + 1) * chunk_size if i != num_threads - 1 else height  # Handle the last chunk which might be bigger\n",
    "        thread = threading.Thread(target=eliminate_flying_pixels_worker, \n",
    "                                  args=(depth_image, ws, threshold, y_start, y_end, result))\n",
    "        threads.append(thread)\n",
    "        thread.start()\n",
    "    \n",
    "    # Wait for all threads to finish\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "    \n",
    "    # Apply the threshold to the depth image\n",
    "    depth_image[result > threshold] = 0\n",
    "    count = np.sum(result > threshold)\n",
    "    print(\"Parallel detected: #\", count)\n",
    "    return depth_image\n",
    "\n",
    "# Dummy data for testing\n",
    "#depth_image = np.random.rand(100, 100)\n",
    "#ws = 2\n",
    "#threshold = 1200\n",
    "\n",
    "# Test the function\n",
    "#output_image = parallel_eliminate_flying_pixels(depth_image, ws, threshold, os.cpu_count())\n",
    "\n",
    "\n",
    "from numba import jit, prange\n",
    "\n",
    "@jit(nopython=True, parallel=True)\n",
    "def numba_eliminate_flying_pixels(depth_image, ws, threshold):\n",
    "    height, width = depth_image.shape\n",
    "    result = np.zeros_like(depth_image, dtype=np.float64)\n",
    "    \n",
    "    for cy in prange(height):\n",
    "        for cx in prange(width):\n",
    "            x_start, x_end = max(0, cx - ws), min(width, cx + ws + 1)\n",
    "            y_start, y_end = max(0, cy - ws), min(height, cy + ws + 1)\n",
    "            window = depth_image[y_start:y_end, x_start:x_end]\n",
    "            result[cy, cx] = np.sum(np.abs(window - depth_image[cy, cx]))\n",
    "    \n",
    "    #for i in prange(height):\n",
    "       # for j in prange(width):\n",
    "      #      if result[i, j] > threshold:\n",
    "     #           depth_image[i, j] = 0\n",
    "    #count = np.sum(result > threshold)\n",
    "    #print(\"Numba detected: #\", count)\n",
    "    return result\n",
    "\n",
    "# Let's time the Numba version\n",
    "\n",
    "#execution_time_numba\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3182d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import convolve2d\n",
    "def fast_s1_with_threshold(depth, WS, threshold):\n",
    "    \"\"\"\n",
    "    Calculate the s1 scores for the entire depth image using convolution and identify flying pixels.\n",
    "    \n",
    "    Parameters:\n",
    "    - depth: 2D numpy array representing the depth image.\n",
    "    - WS: Integer, the window size.\n",
    "    - threshold: Float, the threshold value for identifying flying pixels.\n",
    "    \n",
    "    Returns:\n",
    "    - s1_scores: 2D numpy array of the same shape as depth, containing the s1 scores.\n",
    "    - flying_pixels: 2D numpy array of the same shape as depth, where 1 indicates a flying pixel and 0 otherwise.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a kernel of ones with size (2*WS + 1, 2*WS + 1)\n",
    "    kernel = np.ones((2*WS + 1, 2*WS + 1))\n",
    "    kernel[WS, WS] = 0  # set the center pixel to 0\n",
    "    \n",
    "    # Compute the difference between each pixel and its neighbors\n",
    "    diff = convolve2d(depth, kernel, mode='same', boundary='symm')\n",
    "    \n",
    "    # Compute the s1 scores by summing the absolute differences within the window\n",
    "    s1_scores = convolve2d(np.abs(diff), np.ones((2*WS + 1, 2*WS + 1)), mode='same', boundary='symm')\n",
    "    \n",
    "    # Identify flying pixels based on the threshold\n",
    "    flying_pixels = (s1_scores > threshold).astype(int)\n",
    "    \n",
    "    return s1_scores, flying_pixels\n",
    "\n",
    "# Test the function with a threshold value of 10\n",
    "#test_s1_scores, test_flying_pixels = fast_s1_with_threshold(test_depth_large, 3, 10)\n",
    "\n",
    "# Display the number of identified flying pixels\n",
    "#np.sum(test_flying_pixels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30823c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS REPAIRS FLYING PIXELS\n",
    "def eliminate_flying_pixels_replace_v1(depth_image, ws, threshold):\n",
    "    # Get image size\n",
    "    height, width = depth_image.shape\n",
    "    # Create an empty array for the result\n",
    "    result = np.zeros_like(depth_image, dtype=float)\n",
    "    count = 0\n",
    "    cor = 0\n",
    "    nope=0\n",
    "    # Function to get Euclidean distance between two points\n",
    "    def distance(p1, p2):\n",
    "        return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)\n",
    "\n",
    "    # Iterate over the entire image\n",
    "    for cy in range(height):\n",
    "        for cx in range(width):\n",
    "            # Set the range for the window\n",
    "            x_start, x_end = max(0, cx - ws), min(width, cx + ws + 1)\n",
    "            y_start, y_end = max(0, cy - ws), min(height, cy + ws + 1)\n",
    "            \n",
    "            # Get the window\n",
    "            window = depth_image[y_start:y_end, x_start:x_end]\n",
    "\n",
    "            # Calculate the sum of absolute differences\n",
    "            result[cy, cx] = np.sum(np.abs(window - depth_image[cy, cx]))\n",
    "            if result[cy, cx] > threshold:\n",
    "                count += 1\n",
    "                window_result = result[y_start:y_end, x_start:x_end]\n",
    "\n",
    "                valid_pixels = np.where(window_result <= threshold)\n",
    "                valid_coords = list(zip(*valid_pixels))\n",
    "\n",
    "                if valid_coords:\n",
    "                    distances = [(cy-y)**2 + (cx-x)**2 for y, x in valid_coords]\n",
    "                    nearest_y, nearest_x = valid_coords[np.argmin(distances)]\n",
    "                    depth_image[cy, cx] = 0#depth_image[y_start + nearest_y, x_start + nearest_x]\n",
    "                    cor+=1\n",
    "                else:\n",
    "                    depth_image[cy, cx] = 0\n",
    "                    nope+=1\n",
    "    print(\"Detected: #\",count, \" flying pixels\")   \n",
    "    print(\"Corrected: #\",cor, \" flying pixels\")\n",
    "    print(\"Invalidated: #\",nope, \" flying pixels\")       \n",
    "    return depth_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7d0e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/vigir3d/Datasets/cattle_scans/farm_07_28/Animal_calib_new4//'\n",
    "pcds, tforms = load_filter_pcds(data_path,100000) # lower threshold will remove more points()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46448c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eliminate_flying_pixels_old(depth_image, ws, threshold):\n",
    "   \n",
    "    # Get image size\n",
    "    height, width = depth_image.shape\n",
    "    # Create an empty array for the result\n",
    "    result = np.zeros_like(depth_image, dtype=float)\n",
    "\n",
    "    # Iterate over the entire image\n",
    "    for cy in range(height):\n",
    "        for cx in range(width):\n",
    "            # Set the range for the window\n",
    "            x_start, x_end = max(0, cx - ws), min(width, cx + ws + 1)\n",
    "            y_start, y_end = max(0, cy - ws), min(height, cy + ws + 1)\n",
    "            \n",
    "            # Get the window\n",
    "            window = depth_image[y_start:y_end, x_start:x_end]\n",
    "\n",
    "            # Calculate the sum of absolute differences\n",
    "            result[cy, cx] = np.sum(np.abs(window - depth_image[cy, cx]))\n",
    "    count = np.sum(result > threshold)\n",
    "    print(\"Detected : #\", count, \" flying pixels -old\")\n",
    "    depth_image[result > threshold] = 0\n",
    "    return  depth_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375c2663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAST? CORRECT - 224229\n",
    "from scipy.spatial import cKDTree\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "\n",
    "\n",
    "def eliminate_flying_pixels(depth_image, ws, threshold):\n",
    "    height, width = depth_image.shape\n",
    "    result = np.zeros_like(depth_image, dtype=float)\n",
    "\n",
    "    # Vectorized window-based computation\n",
    "    for cy in range(height):\n",
    "        for cx in range(width):\n",
    "            x_start, x_end = max(0, cx - ws), min(width, cx + ws + 1)\n",
    "            y_start, y_end = max(0, cy - ws), min(height, cy + ws + 1)\n",
    "            \n",
    "            window = depth_image[y_start:y_end, x_start:x_end]\n",
    "            result[cy, cx] = np.sum(np.abs(window - depth_image[cy, cx]))\n",
    "\n",
    "    # Create KD-Tree for faster nearest neighbor search\n",
    "    y_indices, x_indices = np.where(result <= threshold)\n",
    "    valid_points = np.c_[y_indices, x_indices]\n",
    "    tree = cKDTree(valid_points)\n",
    "\n",
    "    # Get the coordinates of the pixels we want to replace\n",
    "    y_bad, x_bad = np.where(result > threshold)\n",
    "    distances, indices = tree.query(np.c_[y_bad, x_bad], k=2)\n",
    "    count = np.sum(result > threshold)\n",
    "    print(\"Detected: #\",count, \" flying pixels\")         \n",
    "\n",
    "\n",
    "    # Replace bad pixels with the values of the nearest valid pixels\n",
    "    depth_image[y_bad, x_bad] = depth_image[y_indices[indices], x_indices[indices]]\n",
    "    return depth_image\n",
    "\n",
    "# IMPROVED?\n",
    "\n",
    "\n",
    "def eliminate_flying_pixels_fast_v1(depth_image, ws, threshold):\n",
    "    # Generate a window kernel\n",
    "    kernel = np.ones((2*ws+1, 2*ws+1))\n",
    "\n",
    "    # Compute the average in the window for each pixel\n",
    "    avg_in_window = ndimage.convolve(depth_image, kernel, mode='constant') / (2*ws+1)**2\n",
    "\n",
    "    # Calculate the absolute differences between each pixel and the window average\n",
    "    abs_diff = np.abs(depth_image - avg_in_window)\n",
    "\n",
    "    # Create a mask for values over the threshold\n",
    "    mask = abs_diff > threshold\n",
    "\n",
    "    # Set flying pixels to zero\n",
    "    depth_image[mask] = 0\n",
    "\n",
    "    return depth_image\n",
    "\n",
    "def eliminate_flying_pixels_fast_v2(depth_image, ws, threshold):\n",
    "    # Generate a window kernel\n",
    "    kernel = np.ones((2*ws+1, 2*ws+1))\n",
    "\n",
    "    # Calculate the sum of absolute differences using convolution\n",
    "    sad = ndimage.convolve(np.abs(depth_image - ndimage.convolve(depth_image, kernel, mode='constant')), kernel, mode='constant')\n",
    "\n",
    "    # Create a mask for values over the threshold\n",
    "    mask = sad > threshold\n",
    "\n",
    "    # Set flying pixels to zero\n",
    "    depth_image[mask] = 0\n",
    "\n",
    "    return depth_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f59de90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def eliminate_flying_pixels_single_loop(depth_image, ws, threshold):\n",
    "    height, width = depth_image.shape\n",
    "    result = np.zeros_like(depth_image, dtype=float)\n",
    "\n",
    "    # Compute the rectangular window limits for every pixel in one go\n",
    "    y_start = np.clip(np.arange(height).reshape(-1, 1) - ws, 0, height)\n",
    "    y_end = np.clip(np.arange(height).reshape(-1, 1) + ws + 1, 0, height)\n",
    "    x_start = np.clip(np.arange(width) - ws, 0, width)\n",
    "    x_end = np.clip(np.arange(width) + ws + 1, 0, width)\n",
    "\n",
    "\n",
    "    # Count of flying pixels\n",
    "    count = 0\n",
    "\n",
    "    for cy in range(height):\n",
    "        for cx in range(width):\n",
    "            window = depth_image[y_start[cy, cx]:y_end[cy, cx], x_start[cy, cx]:x_end[cy, cx]]\n",
    "            result[cy, cx] = np.sum(np.abs(window - depth_image[cy, cx]))\n",
    "\n",
    "            iif result[cy, cx] > threshold:\n",
    "    count += 1\n",
    "    window_y_start, window_y_end = y_start[cy], y_end[cy]\n",
    "    window_x_start, window_x_end = x_start[cx], x_end[cx]\n",
    "    window_result = result[window_y_start:window_y_end, window_x_start:window_x_end]\n",
    "    \n",
    "    valid_pixels = np.where(window_result <= threshold)\n",
    "    valid_coords = list(zip(*valid_pixels))\n",
    "\n",
    "    if valid_coords:\n",
    "        distances = [(cy-y)**2 + (cx-x)**2 for y, x in valid_coords]\n",
    "        nearest_y, nearest_x = valid_coords[np.argmin(distances)]\n",
    "        depth_image[cy, cx] = depth_image[window_y_start + nearest_y, window_x_start + nearest_x]\n",
    "    else:\n",
    "        depth_image[cy, cx] = 0\n",
    "\n",
    "\n",
    "    print(\"Detected: #\", count, \"flying pixels\")\n",
    "    return depth_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65586da7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7107d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS REPAIRS FLYING PIXELS\n",
    "def eliminate_flying_pixels(depth_image, ws, threshold):\n",
    "    # Get image size\n",
    "    height, width = depth_image.shape\n",
    "    # Create an empty array for the result\n",
    "    result = np.zeros_like(depth_image, dtype=float)\n",
    "\n",
    "    # Function to get Euclidean distance between two points\n",
    "    def distance(p1, p2):\n",
    "        return np.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)\n",
    "\n",
    "    # Iterate over the entire image\n",
    "    for cy in range(height):\n",
    "        for cx in range(width):\n",
    "            # Set the range for the window\n",
    "            x_start, x_end = max(0, cx - ws), min(width, cx + ws + 1)\n",
    "            y_start, y_end = max(0, cy - ws), min(height, cy + ws + 1)\n",
    "            \n",
    "            # Get the window\n",
    "            window = depth_image[y_start:y_end, x_start:x_end]\n",
    "\n",
    "            # Calculate the sum of absolute differences\n",
    "            result[cy, cx] = np.sum(np.abs(window - depth_image[cy, cx]))\n",
    "    count = np.sum(result > threshold)\n",
    "    # Loop over the result matrix to replace values beyond the threshold\n",
    "    for cy in range(height):\n",
    "        for cx in range(width):\n",
    "            if result[cy, cx] > threshold:\n",
    "                min_distance = float('inf')\n",
    "                nearest_value = None\n",
    "                \n",
    "                x_start, x_end = max(0, cx - ws), min(width, cx + ws + 1)\n",
    "                y_start, y_end = max(0, cy - ws), min(height, cy + ws + 1)\n",
    "\n",
    "                # Loop over the window to find the nearest valid depth value\n",
    "                for y in range(y_start, y_end):\n",
    "                    for x in range(x_start, x_end):\n",
    "                        if result[y, x] <= threshold and distance((cy, cx), (y, x)) < min_distance:\n",
    "                            min_distance = distance((cy, cx), (y, x))\n",
    "                            nearest_value = depth_image[y, x]\n",
    "                \n",
    "                # If no valid pixel is found in the neighborhood, set the pixel to 0\n",
    "                depth_image[cy, cx] = nearest_value if nearest_value is not None else 0\n",
    "    print(\"Detected: #\",count, \" flying pixels\")         \n",
    "    return depth_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a0f5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_corr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e22965f",
   "metadata": {},
   "outputs": [],
   "source": [
    "correspondences = {}\n",
    "for i, pc_i in enumerate(point_clouds):\n",
    "    for j, pc_j in enumerate(point_clouds):\n",
    "        if i != j:\n",
    "            print(\"Establishing correspondences between: (\", i ,\" and \", j,\")\",)\n",
    "            A_corr, B_corr = compute_pairwise_corrs(pc_i, pc_j, VOXEL_SIZE=0.05)\n",
    "            correspondences[(i, j)] = (A_corr, B_corr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae5d41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2c2f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hi = np.eye(4)\n",
    "#Hi[:3, :3] \n",
    "Hi[:3, 3] = np.array([3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1364461",
   "metadata": {},
   "outputs": [],
   "source": [
    "Hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f7bdbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_point_clouds = len(point_clouds)\n",
    "homogeneous_transformations = [np.eye(4) for _ in range(num_point_clouds)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a58d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.transform import Rotation as R\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "def robust_loss(e,mu):\n",
    "    return  mu * e**2 / (mu**2 + e**2)\n",
    "\n",
    "def objective_function(H, X, correspondences,mu):\n",
    "    homogeneous_transformations = unpack_params(H)\n",
    "    J = 0\n",
    "   \n",
    "    for (i, j), (A_corr, B_corr) in correspondences.items():\n",
    "        w_H_i = homogeneous_transformations[i]\n",
    "        w_H_j = homogeneous_transformations[j]\n",
    "        C_i = np.vstack((A_corr, np.ones(A_corr.shape[1]))) # Homogeneous coordinates\n",
    "        C_j = np.vstack((B_corr, np.ones(B_corr.shape[1]))) # Homogeneous coordinates\n",
    "\n",
    "        transformed_C_i = w_H_i.dot(C_i)\n",
    "        transformed_C_j_inv = np.linalg.inv(w_H_j).dot(C_j)\n",
    "\n",
    "        error = transformed_C_j_inv - transformed_C_i\n",
    "        error = np.sum(error[:3] ** 2, axis=0) # Ignoring homogeneous component, summing squared differences\n",
    "\n",
    "        J += np.sum(robust_loss(error,mu))\n",
    "        #print(J)\n",
    "\n",
    "    return J\n",
    "\n",
    "\n",
    "def error_euc_constrained(params, X, corrs, mu):\n",
    "    H = unpack_params(params)\n",
    "    res = []\n",
    "    for i in range(len(X)):\n",
    "        for j in range(len(X)):\n",
    "            if i == j:\n",
    "                continue\n",
    "            \n",
    "            iHj = np.linalg.inv(H[i]) @ H[j]\n",
    "            Xi_est = iHj[:3, :3] @ X[j] + iHj[:3, 3, None]\n",
    "\n",
    "            xi_est = Xi_est[:, corrs[i][j][:, 1]]\n",
    "            xi = X[i][:, corrs[i][j][:, 0]]\n",
    "            e = xi - xi_est\n",
    "            res_i = mu * e**2 / (mu**2 + e**2)\n",
    "            res.extend(res_i.ravel())\n",
    "    return np.array(res)\n",
    "\n",
    "def unpack_params(params): # From EULER to HTM\n",
    "    H = [np.eye(4)]\n",
    "    for i in range(0, len(params), 6):\n",
    "        rpy = params[i:i+3]\n",
    "        t = params[i+3:i+6]\n",
    "        rotation_matrix = R.from_euler('zyx', rpy, degrees=False).as_matrix()\n",
    "        Hi = np.eye(4)\n",
    "        Hi[:3, :3] = rotation_matrix\n",
    "        Hi[:3, 3] = t\n",
    "        H.append(Hi)\n",
    "    return H\n",
    "\n",
    "def pack_params(H): # TO EULER\n",
    "    params = []\n",
    "    for i in range(1, len(H)):\n",
    "        rotation_matrix = H[i][:3, :3]\n",
    "        rpy = R.from_matrix(rotation_matrix).as_euler('zyx', degrees=False)\n",
    "        t = H[i][:3, 3]\n",
    "        params.extend(list(rpy) + list(t))\n",
    "    return params\n",
    "\n",
    "def bundle_registration_6dof(H,X, corrs):\n",
    "    options = {'ftol': 1e-5, 'xtol': 1e-3, 'verbose': 1}\n",
    "    mu_values = [1, 0.7143, 0.5102, 0.3644, 0.2603, 0.1859, 0.1328, 0.0949, 0.0678, 0.0484, 0.0346, 0.0247]\n",
    "\n",
    "    for mu in mu_values:\n",
    "        input_params = pack_params(H)\n",
    "        result = least_squares(objective_function, input_params, args=(X, corrs,mu), **options)\n",
    "        H = unpack_params(result.x)\n",
    "    return H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b89cf3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "homogeneous_transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d998f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "htms_init[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e89229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "htms_init = [H0,H1,np.eye(4),H3,H4,H5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dec0ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd  = pack_params(htms_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893c22ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_set = bundle_registration_6dof(htms_init,X, correspondences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305a557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "H_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2547fd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q0 = copy.deepcopy(p0).transform(H_set[0])\n",
    "q1 = copy.deepcopy(p1).transform(H_set[1])\n",
    "q3 = copy.deepcopy(p3).transform(H_set[3])\n",
    "q4 = copy.deepcopy(p4).transform(H_set[4])\n",
    "q5 = copy.deepcopy(p5).transform(H_set[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abb13dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([q0,q1,q3,q4,q5,boxes[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853718fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pack_params(homogeneous_transformations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e263c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [np.asarray(pcd.points).T for pcd in point_clouds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5adf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = bundle_registration_6dof(homogeneous_transformations,X,correspondences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92d7797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pairwise_corrs(p0,p1,VOXEL_SIZE=0.05):    \n",
    "    VOXEL_SIZE = 0.05\n",
    "    pp0 = p0.voxel_down_sample(voxel_size=VOXEL_SIZE)\n",
    "    pp1 = p1.voxel_down_sample(voxel_size=VOXEL_SIZE)\n",
    "\n",
    "    A_xyz = pcd2xyz(pp0) # np array of size 3 by N\n",
    "    B_xyz = pcd2xyz(pp1) # np array of size 3 by M\n",
    "\n",
    "\n",
    "    # The XYZ points are supposed to be the matches, lets transpose and find the mutual nearest euclidean neighbors\n",
    "    A_feats = A_xyz.T\n",
    "    B_feats = B_xyz.T\n",
    "\n",
    "    #print(A_feats.shape)\n",
    "    #print(\"Computing Nearest Neighbor correspondences\")\n",
    "    # establish correspondences by nearest neighbour search in feature space\n",
    "    corrs_A, corrs_B = find_correspondences(\n",
    "        A_feats, B_feats, mutual_filter=True)\n",
    "    A_corr = A_xyz[:,corrs_A] # np array of size 3 by num_corrs\n",
    "    B_corr = B_xyz[:,corrs_B] # np array of size 3 by num_corrs\n",
    "\n",
    "    num_corrs = A_corr.shape[1]\n",
    "    print(f'NN generates {num_corrs} putative correspondences.')\n",
    "    return A_corr, B_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8feddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the point clouds together with feature correspondences\n",
    "points = np.concatenate((A_corr.T,B_corr.T),axis=0)\n",
    "lines = []\n",
    "for i in range(num_corrs):\n",
    "    lines.append([i,i+num_corrs])\n",
    "colors = [[0, 1, 0] for i in range(len(lines))] # lines are shown in green\n",
    "line_set = o3d.geometry.LineSet(\n",
    "    points=o3d.utility.Vector3dVector(points),\n",
    "    lines=o3d.utility.Vector2iVector(lines),\n",
    ")\n",
    "line_set.colors = o3d.utility.Vector3dVector(colors)\n",
    "o3d.visualization.draw_geometries([p0,p1,line_set])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2430854",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9257d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r,h01 = register_two_views_teaser(boxes[0],boxes[1],0.05)\n",
    "r,h12 = register_two_views_teaser(boxes[1],boxes[2],0.05)\n",
    "r,h32 = register_two_views_teaser(boxes[3],boxes[2],0.05)\n",
    "r,h43 = register_two_views_teaser(boxes[4],boxes[3],0.05)\n",
    "r,h52 = register_two_views_teaser(boxes[5],boxes[2],0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5763d8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115e2b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "t01 = colored_ICP(boxes[0],boxes[1])\n",
    "draw_registration_result(boxes[0],boxes[1],t01)\n",
    "t12 = colored_ICP(boxes[1],boxes[2])\n",
    "draw_registration_result(boxes[1],boxes[2],t12)\n",
    "t32 = colored_ICP(boxes[3],boxes[2])\n",
    "draw_registration_result(boxes[3],boxes[2],t32)\n",
    "t43 = colored_ICP(boxes[4],boxes[3])\n",
    "draw_registration_result(boxes[4],boxes[3],t43)\n",
    "t52 = colored_ICP(boxes[5],boxes[2])\n",
    "draw_registration_result(boxes[5],boxes[2],t52)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1094cd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_inlier_outlier(cloud, ind):\n",
    "    inlier_cloud = cloud.select_by_index(ind)\n",
    "    outlier_cloud = cloud.select_by_index(ind, invert=True)\n",
    "\n",
    "    print(\"Showing outliers (red) and inliers (gray): \")\n",
    "    outlier_cloud.paint_uniform_color([1, 0, 0])\n",
    "    inlier_cloud.paint_uniform_color([0.8, 0.8, 0.8])\n",
    "    o3d.visualization.draw_geometries([inlier_cloud, outlier_cloud])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f615cc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Statistical oulier removal\")\n",
    "cl, ind = pbc.remove_statistical_outlier(nb_neighbors=2,\n",
    "                                                    std_ratio=0.5)\n",
    "display_inlier_outlier(pbc, ind)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf66d099",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13edf6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([cl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de12a41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([pbc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1948cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(boxes)):\n",
    "    o3d.visualization.draw_geometries([boxes[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba2422a",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([p0,p1,boxes[2],p3,p4,p5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6ea329",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_clouds = [p0,p1,boxes[2],p3,p4,p5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2f95e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "kdtrees = [o3d.geometry.KDTreeFlann(pc) for pc in point_clouds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c38f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_knn_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08431b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e116c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad684f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,z = kdtrees[0].search_knn_vector_3d([0,0,0],10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09097925",
   "metadata": {},
   "outputs": [],
   "source": [
    "H0 = t12 @ t01\n",
    "H1 = t12\n",
    "H3 = t32\n",
    "H4 = t32 @ t43\n",
    "H5 = t52\n",
    "\n",
    "p0 = copy.deepcopy(boxes[0]).transform(H0)\n",
    "p1 = copy.deepcopy(boxes[1]).transform(H1)\n",
    "p3 = copy.deepcopy(boxes[3]).transform(H3)\n",
    "p4 = copy.deepcopy(boxes[4]).transform(H4)\n",
    "p5 = copy.deepcopy(boxes[5]).transform(H5)\n",
    "\n",
    "o3d.visualization.draw_geometries([p0,p1,boxes[2],p3,p4,p5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c5ad27",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_c1_combined = o3d.geometry.PointCloud()\n",
    "box_c1_combined = p0+p1+boxes[2]+p3+p4+p5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7696880",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_and_orient_normals(pcd, voxel_size):\n",
    "    normal_radius = voxel_size*2.0\n",
    "    pcd.estimate_normals(\n",
    "    search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=normal_radius, max_nn=30))\n",
    "    pcd.orient_normals_towards_camera_location()\n",
    "    return pcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8881dd",
   "metadata": {},
   "outputs": [],
   "source": [
    " o3d.io.write_point_cloud(box_path+\"/c1.ply\", pbc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff50dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_cleaned_down = pcd_cleaned.voxel_down_sample(0.05)\n",
    "o3d.visualization.draw_geometries([pcd_cleaned_down])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76ab4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a668ab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('run Poisson surface reconstruction')\n",
    "\n",
    "with o3d.utility.VerbosityContextManager(\n",
    "        o3d.utility.VerbosityLevel.Debug) as cm:\n",
    "    mesh, densities = o3d.geometry.TriangleMesh.create_from_point_cloud_poisson(\n",
    "        pcd_cleaned_down, depth=8)\n",
    "o3d.visualization.draw_geometries([mesh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adab67f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c977f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_cleaned = o3d.io.read_point_cloud(\"/home/vigir3d/Desktop/c1_cleaned.ply\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e193f958",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([pcd_cleaned])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7abda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pbc = compute_and_orient_normals(box_c1_combined,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a8c6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_registration_result_original_color(source, target, transformation):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    source_temp.transform(transformation)\n",
    "    o3d.visualization.draw_geometries([source_temp, target])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec7474",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_registration_result_original_color(src,tgt,t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64cc7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "r,t = register_two_views_ateaser(src,tgt,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d7357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "src = copy.deepcopy(pcds[0])\n",
    "tgt = copy.deepcopy(pcds[1])\n",
    "\n",
    "\n",
    "radius = 0.01\n",
    "print(\"3-2. Estimate normal.\")\n",
    "src.estimate_normals(\n",
    "    o3d.geometry.KDTreeSearchParamHybrid(radius=radius * 2, max_nn=30))\n",
    "\n",
    "tgt.estimate_normals(\n",
    "    o3d.geometry.KDTreeSearchParamHybrid(radius=radius * 2, max_nn=30))\n",
    "\n",
    "\n",
    "\n",
    "sigma = 0.1\n",
    "loss = o3d.pipelines.registration.GMLoss(k=sigma)\n",
    "print(\"Using robust loss:\", loss)\n",
    "p2l = o3d.pipelines.registration.TransformationEstimationPointToPlane(loss)\n",
    "\n",
    "\n",
    "threshold = 0.1\n",
    "reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "     src, tgt, threshold, np.eye(4), p2l,\n",
    "    o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=2000))\n",
    "print(reg_p2p)\n",
    "print(\"Transformation is:\")\n",
    "print(reg_p2p.transformation)\n",
    "draw_registration_result(pcds[0], pcds[1], reg_p2p.transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b749da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# register pairs and add:\n",
    "r,t = register_two_views_teaser(pcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c99d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Created \" ,len(pcds), \" point clouds\")\n",
    "\n",
    "t01= colored_ICP(pcds[0],pcds[1])\n",
    "t52= colored_ICP(pcds[5],pcds[2])\n",
    "t43= colored_ICP(pcds[4],pcds[3])\n",
    "t12= colored_ICP(pcds[1],pcds[2])\n",
    "t32= colored_ICP(pcds[3],pcds[2])\n",
    "\n",
    "\n",
    "H0 = t12 @ t01\n",
    "H1 = t12\n",
    "H3 = t32\n",
    "H4 = t32 @ t43\n",
    "H5 = t52\n",
    "\n",
    "\n",
    "np.savetxt(\"c_icp_0_2.txt\", H0)\n",
    "np.savetxt(\"c_icp_1_2.txt\", H1)\n",
    "np.savetxt(\"c_icp_3_2.txt\", H3)\n",
    "np.savetxt(\"c_icp_4_2.txt\", H4)\n",
    "np.savetxt(\"c_icp_5_2.txt\", H5)\n",
    "\n",
    "p0 = copy.deepcopy(pcds[0]).transform(H0)\n",
    "p1 = copy.deepcopy(pcds[1]).transform(H1)\n",
    "p3 = copy.deepcopy(pcds[3]).transform(H3)\n",
    "p4 = copy.deepcopy(pcds[4]).transform(H4)\n",
    "p5 = copy.deepcopy(pcds[5]).transform(H5)\n",
    "\n",
    "o3d.visualization.draw_geometries([p0,p1,pcds[2],p3,p4,p5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c097e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bilateral_filter(depth_image):\n",
    "    # Assuming depth_image is your depth image as a numpy array\n",
    "    # Make sure to convert it to an appropriate format (e.g., uint8) if it's not already\n",
    "    depth_image = (depth_image / np.max(depth_image) * 255).astype(np.uint8)\n",
    "\n",
    "    # Apply the bilateral filter\n",
    "    filtered_image = cv2.bilateralFilter(depth_image, d=9, sigmaColor=75, sigmaSpace=75)\n",
    "\n",
    "    # If you need the filtered depth map in its original range, you can scale it back:\n",
    "    filtered_image = (filtered_image / 255.0 * np.max(depth_image)).astype(depth_image.dtype)\n",
    "    return filtered_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41622c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpath = '/home/vigir3d/Datasets/cattle_scans/farm_07_28/Animal_cyl10/'\n",
    "bpcds = generate_pcds(bpath,tforms,True)\n",
    "for i in range(len(bpcds)):\n",
    "   \n",
    "    o3d.visualization.draw_geometries([bpcds[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09b83d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries(bpcds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe89f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r,t = register_two_views_teaser(bpcds[0],bpcds[1],0.05)\n",
    "r,t = register_two_views_teaser(bpcds[1],bpcds[2],0.05)\n",
    "r,t = register_two_views_teaser(bpcds[5],bpcds[2],0.05)\n",
    "r,t = register_two_views_teaser(bpcds[4],bpcds[3],0.05)\n",
    "r,t = register_two_views_teaser(bpcds[3],bpcds[5],0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7a2bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "t01= colored_ICP(bpcds[0],bpcds[1])\n",
    "t52= colored_ICP(bpcds[5],bpcds[2])\n",
    "t43= colored_ICP(bpcds[4],bpcds[3])\n",
    "t12= colored_ICP(bpcds[1],bpcds[2])\n",
    "t32= colored_ICP(bpcds[3],bpcds[2])\n",
    "\n",
    "\n",
    "H0 = t12 @ t01\n",
    "H1 = t12\n",
    "H3 = t32\n",
    "H4 = t32 @ t43\n",
    "H5 = t52\n",
    "\n",
    "\n",
    "p0 = copy.deepcopy(bpcds[0]).transform(H0)\n",
    "p1 = copy.deepcopy(bpcds[1]).transform(H1)\n",
    "p3 = copy.deepcopy(bpcds[3]).transform(H3)\n",
    "p4 = copy.deepcopy(bpcds[4]).transform(H4)\n",
    "p5 = copy.deepcopy(bpcds[5]).transform(H5)\n",
    "\n",
    "o3d.visualization.draw_geometries([p0,p1,bpcds[2],p3,p4,p5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f3e909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def colored_ICP(source, target):\n",
    "    #draw_registration_result(source, target, np.eye(4))\n",
    "    voxel_radius = [0.04, 0.02, 0.01]\n",
    "    max_iter = [50, 30, 14]\n",
    "    current_transformation = np.identity(4)\n",
    "    print(\"3. Colored point cloud registration\")\n",
    "    for scale in range(3):\n",
    "        iters = max_iter[scale]\n",
    "        radius = voxel_radius[scale]\n",
    "        print(\"iteration: \", iters, radius, scale)\n",
    "\n",
    "        print(\"3-1. Downsample with a voxel size %.2f\" % radius)\n",
    "        source_down = copy.deepcopy(source).voxel_down_sample(radius)\n",
    "        target_down = copy.deepcopy(target).voxel_down_sample(radius)\n",
    "\n",
    "        print(\"3-2. Estimate normal.\")\n",
    "        source_down.estimate_normals(\n",
    "            o3d.geometry.KDTreeSearchParamHybrid(radius=radius * 2, max_nn=30))\n",
    "        target_down.estimate_normals(\n",
    "            o3d.geometry.KDTreeSearchParamHybrid(radius=radius * 2, max_nn=30))\n",
    "\n",
    "        print(\"3-3. Applying colored point cloud registration\")\n",
    "        result_icp = o3d.pipelines.registration.registration_colored_icp(\n",
    "            source_down, target_down, radius, current_transformation,\n",
    "            o3d.pipelines.registration.TransformationEstimationForColoredICP(),\n",
    "            o3d.pipelines.registration.ICPConvergenceCriteria(relative_fitness=1e-6,\n",
    "                                                              relative_rmse=1e-6,\n",
    "                                                              max_iteration=iters))\n",
    "        current_transformation = result_icp.transformation\n",
    "    \n",
    "    return current_transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10228a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Apply the ICP results to new data\n",
    "\n",
    "\n",
    "def generate_pcds(data_path, tforms, apply_tforms=True):  # returns a list of  pcds\n",
    "    reader = o3d.io.AzureKinectMKVReader()\n",
    "    abspath = data_path\n",
    "    \n",
    "    files = glob.glob(data_path+'/*.mkv')\n",
    "    files.sort()\n",
    "    print(\"There are : \", len(files), \"present here!\")\n",
    "\n",
    "    list_size = len(files)\n",
    "    pcds = [None] * list_size\n",
    "\n",
    "    for i in range(len(files)): # for each view\n",
    "        inFile = files[i]\n",
    "        fname = inFile.split('/')[-1]\n",
    "        file_name = fname.split('.mkv')[0]\n",
    "        print(\"Current File: \", file_name)\n",
    "\n",
    "        reader.open(inFile)\n",
    "        if not reader.is_opened():\n",
    "            raise RuntimeError(\"Unable to open file {}\".format(inFile))\n",
    "        metadata = reader.get_metadata()\n",
    "  \n",
    "        # write the metadata to a JSON file since that seems to be the only\n",
    "        # way to retrieve that data\n",
    "        o3d.io.write_azure_kinect_mkv_metadata(\n",
    "                    '{}/{}_intrinsic.json'.format(abspath,file_name), metadata)\n",
    "\n",
    "        # Open the file and load the JSON\n",
    "        with open(abspath+\"/\" + file_name + \"_intrinsic.json\") as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        height = data['height']\n",
    "        width = data['width']\n",
    "        intrinsics = data[\"intrinsic_matrix\"]\n",
    "        camera_intrinsics = o3d.camera.PinholeCameraIntrinsic()\n",
    "        cx = intrinsics[6]\n",
    "        cy = intrinsics[7]\n",
    "        fx = intrinsics[0]\n",
    "        fy = intrinsics[4]\n",
    "        camera_intrinsics.set_intrinsics(width,height,fx,fy,cx,cy)\n",
    "        K = (fx,fy,cx,cy)\n",
    "\n",
    "        last_frame = None\n",
    "        while not reader.is_eof(): # go until hitting eof because of exposure issues in early color frames\n",
    "            rgbda = reader.next_frame()\n",
    "            if rgbda is None:\n",
    "                print(\"Got nothing! \")\n",
    "                continue\n",
    "            last_frame = rgbda\n",
    "\n",
    "        if last_frame is not None:\n",
    "            print(\"Got the last frame\")\n",
    "        else:\n",
    "            print(\"************No valid frames found in the .mkv file.**********\")\n",
    "            \n",
    "        #depth_image_array = np.asarray(last_frame.depth)\n",
    "        #filtered_depth = eliminate_flying_pixels(depth_image_array, ws, flying_pixel_filter_threshold)\n",
    "\n",
    "        #masked_rgbd = segment_images(last_frame)\n",
    "        \n",
    "        #pcd = backproject_o3d(masked_rgbd, camera_intrinsics)\n",
    "        filt_pcd = backproject_o3d(last_frame, camera_intrinsics)\n",
    "        #o3d.visualization.draw_geometries([pcd])\n",
    "        #o3d.visualization.draw_geometries([filt_pcd])\n",
    "        if apply_tforms:\n",
    "            pcds[i] = copy.deepcopy(filt_pcd).transform(tforms[i])\n",
    "        else :\n",
    "            pcds[i] = filt_pcd\n",
    "    return pcds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1aab5b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5766552c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_sort_files(path):\n",
    "    # Get a list of all files that start with 'H_' and end with '.txt'\n",
    "    files = glob.glob(os.path.join(path, 'H_*.txt'))\n",
    "    \n",
    "    # Sort the list of files\n",
    "    files.sort()\n",
    "    \n",
    "    # Load the sorted files as NumPy arrays and store them in a list\n",
    "    arrays = [np.loadtxt(file) for file in files]\n",
    "    \n",
    "    return arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b9f785",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9855b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09d30009",
   "metadata": {},
   "outputs": [],
   "source": [
    "H[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ced317a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tforms_apriltag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m calib_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/vigir3d/Software/programs/Cattle_Scanner\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#H = read_sort_files(calib_path)\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#transforms = [H0,H1,np.eye(4), H3, H4, H5]\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m boxes \u001b[38;5;241m=\u001b[39m generate_pcds(data_path_boxes,\u001b[43mtforms_apriltag\u001b[49m,\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tforms_apriltag' is not defined"
     ]
    }
   ],
   "source": [
    "data_path_boxes = '/home/vigir3d/Datasets/cattle_scans/farm_07_24/Animal_box_1'\n",
    "calib_path = '/home/vigir3d/Software/programs/Cattle_Scanner'\n",
    "#H = read_sort_files(calib_path)\n",
    "#transforms = [H0,H1,np.eye(4), H3, H4, H5]\n",
    "boxes = generate_pcds(data_path_boxes,tforms_apriltag,True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d36875f1",
   "metadata": {},
   "source": [
    "o3d.visualization.draw_geometries(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444c5160",
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes_combined = o3d.geometry.PointCloud()\n",
    "for box in boxes :\n",
    "    boxes_combined+=box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6696c678",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055136a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.io.write_point_cloud(\"boxes_1.ply\",boxes_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f834188e",
   "metadata": {},
   "outputs": [],
   "source": [
    "E0 = h12 @ h01\n",
    "E1 = h12\n",
    "E3 = h32\n",
    "E4 = h32 @ h43\n",
    "E5 = h52\n",
    "pcds_combined = o3d.geometry.PointCloud()\n",
    "\n",
    "b0 = copy.deepcopy(boxes[0]).transform(E0)\n",
    "pcds_combined = b0\n",
    "o3d.visualization.draw_geometries([pcds_combined])\n",
    "b1 = copy.deepcopy(boxes[1]).transform(E1)\n",
    "pcds_combined +=b1\n",
    "o3d.visualization.draw_geometries([pcds_combined])\n",
    "pcds_combined +=boxes[2]\n",
    "o3d.visualization.draw_geometries([pcds_combined])\n",
    "b3 = copy.deepcopy(boxes[3]).transform(E3)\n",
    "pcds_combined +=b3\n",
    "o3d.visualization.draw_geometries([pcds_combined])\n",
    "b4 = copy.deepcopy(boxes[4]).transform(E4)\n",
    "pcds_combined +=b4\n",
    "o3d.visualization.draw_geometries([pcds_combined])\n",
    "b5 = copy.deepcopy(boxes[5]).transform(E5)\n",
    "pcds_combined +=b5\n",
    "o3d.visualization.draw_geometries([pcds_combined])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0078a65d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c52ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "r, h01 = register_two_views_teaser(boxes[0],boxes[1],0.05)\n",
    "r, h12 = register_two_views_teaser(boxes[1],boxes[2],0.05)\n",
    "r, h32 = register_two_views_teaser(boxes[3],boxes[2],0.05)\n",
    "r, h43 = register_two_views_teaser(boxes[4],boxes[3],0.05)\n",
    "r, h52 = register_two_views_teaser(boxes[5],boxes[2],0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c5b4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "b01= colored_ICP(boxes[0],boxes[1])\n",
    "b52= colored_ICP(boxes[5],boxes[2])\n",
    "b43= colored_ICP(boxes[4],boxes[3])\n",
    "b12= colored_ICP(boxes[1],boxes[2])\n",
    "b32= colored_ICP(boxes[3],boxes[2])\n",
    "\n",
    "\n",
    "T0 = b12 @ b01\n",
    "T1 = b12\n",
    "T3 = b32\n",
    "T4 = b32 @ b43\n",
    "T5 = b52\n",
    "\n",
    "pcds_combined = o3d.geometry.PointCloud()\n",
    "\n",
    "b0 = copy.deepcopy(boxes[0]).transform(T0)\n",
    "pcds_combined = b0\n",
    "o3d.visualization.draw_geometries([pcds_combined])\n",
    "b1 = copy.deepcopy(boxes[1]).transform(T1)\n",
    "pcds_combined +=b1\n",
    "o3d.visualization.draw_geometries([pcds_combined])\n",
    "pcds_combined +=boxes[2]\n",
    "o3d.visualization.draw_geometries([pcds_combined])\n",
    "b3 = copy.deepcopy(boxes[3]).transform(T3)\n",
    "pcds_combined +=b3\n",
    "o3d.visualization.draw_geometries([pcds_combined])\n",
    "b4 = copy.deepcopy(boxes[4]).transform(T4)\n",
    "pcds_combined +=b4\n",
    "o3d.visualization.draw_geometries([pcds_combined])\n",
    "b5 = copy.deepcopy(boxes[5]).transform(T5)\n",
    "pcds_combined +=b5\n",
    "o3d.visualization.draw_geometries([pcds_combined])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1503963",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([pcd01])\n",
    "o3d.visualization.draw_geometries([pcd43])\n",
    "o3d.visualization.draw_geometries([pcd52])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb178a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd01 = combine_pairs(boxes[0], boxes[1], b01)\n",
    "pcd43 = combine_pairs(boxes[4], boxes[3], b43)\n",
    "pcd52 = combine_pairs(boxes[5], boxes[2], b52)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5036c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd0152 = combine_pairs(pcd01,pcd52,t12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263b9acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69522544",
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = colored_ICP(pcd43,boxes[5])\n",
    "draw_registration_result(pcd43,boxes[5],tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193e44e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#r12,t12 = register_two_views_teaser(pcd01,pcd52,0.08)\n",
    "#tt = colored_ICP(pcd43,pcd0152)\n",
    "#draw_registration_result(pcd43,pcd0152,tt)\n",
    "for vs in np.arange(0.01,0.1,0.01):\n",
    "    print(\"***\", vs)\n",
    "    r32,t32 = register_two_views_teaser(pcd43,boxes[5],vs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068b7ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_pairs(src,target,T):\n",
    "    pcd_combined_pair = o3d.geometry.PointCloud()\n",
    "    pcd_combined_pair = copy.deepcopy(src).transform(T) + target\n",
    "    return pcd_combined_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47d6238",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
