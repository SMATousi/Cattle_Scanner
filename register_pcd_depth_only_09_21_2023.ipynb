{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89580a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "import json\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import teaserpp_python\n",
    "from numpy.linalg import inv\n",
    "from scipy.spatial import cKDTree\n",
    "import time\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision import transforms as T \n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "from numba import jit, prange\n",
    "import os\n",
    "import csv \n",
    "# Load MaskRCNN \n",
    "\n",
    "def register_two_views_teaser(A_pcd_raw,B_pcd_raw,VOXEL_SIZE):\n",
    "    \n",
    "    VISUALIZE = True\n",
    "    A_pcd = A_pcd_raw.voxel_down_sample(voxel_size=VOXEL_SIZE)\n",
    "    B_pcd = B_pcd_raw.voxel_down_sample(voxel_size=VOXEL_SIZE)\n",
    "    #if VISUALIZE:\n",
    "     #   o3d.visualization.draw_geometries([A_pcd,B_pcd]) # plot downsampled A and B \n",
    "\n",
    "    A_xyz = pcd2xyz(A_pcd) # np array of size 3 by N\n",
    "    B_xyz = pcd2xyz(B_pcd) # np array of size 3 by M\n",
    "\n",
    "    print(\"Extracting FPFH features\")\n",
    "    # extract FPFH features\n",
    "    A_feats = extract_fpfh(A_pcd,VOXEL_SIZE)\n",
    "    B_feats = extract_fpfh(B_pcd,VOXEL_SIZE)\n",
    "    print(A_feats.shape)\n",
    "    print(\"Computing FPFH correspondences\")\n",
    "    # establish correspondences by nearest neighbour search in feature space\n",
    "    corrs_A, corrs_B = find_correspondences(\n",
    "        A_feats, B_feats, mutual_filter=True)\n",
    "    A_corr = A_xyz[:,corrs_A] # np array of size 3 by num_corrs\n",
    "    B_corr = B_xyz[:,corrs_B] # np array of size 3 by num_corrs\n",
    "\n",
    "    num_corrs = A_corr.shape[1]\n",
    "    print(f'FPFH generates {num_corrs} putative correspondences.')\n",
    "\n",
    "    # visualize the point clouds together with feature correspondenc\n",
    "    # robust global registration using TEASER++\n",
    "    NOISE_BOUND = VOXEL_SIZE\n",
    "    teaser_solver = get_teaser_solver(NOISE_BOUND)\n",
    "    teaser_solver.solve(A_corr,B_corr)\n",
    "    solution = teaser_solver.getSolution()\n",
    "    R_teaser = solution.rotation\n",
    "    t_teaser = solution.translation\n",
    "    T_teaser = Rt2T(R_teaser,t_teaser)\n",
    "\n",
    "    # Visualize the registration results\n",
    "    A_pcd_T_teaser = copy.deepcopy(A_pcd).transform(T_teaser)\n",
    "    #o3d.visualization.draw_geometries([A_pcd_T_teaser,B_pcd])\n",
    "\n",
    "    # local refinement using ICP\n",
    "    icp_sol = o3d.pipelines.registration.registration_icp(\n",
    "          A_pcd, B_pcd, NOISE_BOUND, T_teaser,\n",
    "          o3d.pipelines.registration.TransformationEstimationPointToPoint(),\n",
    "          o3d.pipelines.registration.ICPConvergenceCriteria(max_iteration=100))\n",
    "    T_icp = icp_sol.transformation\n",
    "\n",
    "    # visualize the registration after ICP refinement\n",
    "    A_pcd_T_icp = copy.deepcopy(A_pcd).transform(T_icp)\n",
    "    if VISUALIZE:\n",
    "        Acopy = copy.deepcopy(A_pcd_T_icp).paint_uniform_color([0.0,0.0,1])\n",
    "        Bcopy = copy.deepcopy(B_pcd).paint_uniform_color([1.0,0.0,0.0])\n",
    "        o3d.visualization.draw_geometries([Acopy,Bcopy])\n",
    "    tformed_A = copy.deepcopy(A_pcd_raw).transform(T_icp)\n",
    "    res = o3d.geometry.PointCloud()\n",
    "    res = tformed_A + B_pcd_raw\n",
    "    \n",
    "    return res,T_icp\n",
    "\n",
    "def pcd2xyz(pcd):\n",
    "    return np.asarray(pcd.points).T\n",
    "\n",
    "def extract_fpfh(pcd, voxel_size):\n",
    "    radius_normal = voxel_size * 2\n",
    "    pcd.estimate_normals(\n",
    "      o3d.geometry.KDTreeSearchParamHybrid(radius=radius_normal, max_nn=30))\n",
    "\n",
    "    radius_feature = voxel_size * 5\n",
    "    fpfh = o3d.pipelines.registration.compute_fpfh_feature(\n",
    "      pcd, o3d.geometry.KDTreeSearchParamHybrid(radius=radius_feature, max_nn=100))\n",
    "    return np.array(fpfh.data).T\n",
    "\n",
    "def find_knn_cpu(feat0, feat1, knn=1, return_distance=False):\n",
    "    feat1tree = cKDTree(feat1)\n",
    "    dists, nn_inds = feat1tree.query(feat0, k=knn, workers=10)\n",
    "    if return_distance:\n",
    "        return nn_inds, dists\n",
    "    else:\n",
    "        return nn_inds\n",
    "\n",
    "def find_correspondences(feats0, feats1, mutual_filter=True):\n",
    "    nns01 = find_knn_cpu(feats0, feats1, knn=1, return_distance=False)\n",
    "    corres01_idx0 = np.arange(len(nns01))\n",
    "    corres01_idx1 = nns01\n",
    "\n",
    "    if not mutual_filter:\n",
    "        return corres01_idx0, corres01_idx1\n",
    "\n",
    "    nns10 = find_knn_cpu(feats1, feats0, knn=1, return_distance=False)\n",
    "    corres10_idx1 = np.arange(len(nns10))\n",
    "    corres10_idx0 = nns10\n",
    "\n",
    "    mutual_filter = (corres10_idx0[corres01_idx1] == corres01_idx0)\n",
    "    corres_idx0 = corres01_idx0[mutual_filter]\n",
    "    corres_idx1 = corres01_idx1[mutual_filter]\n",
    "\n",
    "    return corres_idx0, corres_idx1\n",
    "\n",
    "def get_teaser_solver(noise_bound):\n",
    "    solver_params = teaserpp_python.RobustRegistrationSolver.Params()\n",
    "    solver_params.cbar2 = 1.0\n",
    "    solver_params.noise_bound = noise_bound\n",
    "    solver_params.estimate_scaling = False\n",
    "    solver_params.inlier_selection_mode = \\\n",
    "        teaserpp_python.RobustRegistrationSolver.INLIER_SELECTION_MODE.PMC_EXACT\n",
    "    solver_params.rotation_tim_graph = \\\n",
    "        teaserpp_python.RobustRegistrationSolver.INLIER_GRAPH_FORMULATION.CHAIN\n",
    "    solver_params.rotation_estimation_algorithm = \\\n",
    "        teaserpp_python.RobustRegistrationSolver.ROTATION_ESTIMATION_ALGORITHM.GNC_TLS\n",
    "    solver_params.rotation_gnc_factor = 1.4\n",
    "    solver_params.rotation_max_iterations = 10000\n",
    "    solver_params.rotation_cost_threshold = 1e-16\n",
    "    solver = teaserpp_python.RobustRegistrationSolver(solver_params)\n",
    "    return solver\n",
    "\n",
    "def Rt2T(R,t):\n",
    "    T = np.identity(4)\n",
    "    T[:3,:3] = R\n",
    "    T[:3,3] = t\n",
    "    return T \n",
    "\n",
    "def save_results(animal_id, data_path, mesh, pcd_downsampled, sa, v):\n",
    "    # Save the mesh and PCD\n",
    "    mesh_filename = os.path.join(data_path, f\"{animal_id}_mesh.ply\")\n",
    "    pcd_filename = os.path.join(data_path, f\"{animal_id}_pcd_downsampled.ply\")\n",
    "\n",
    "    o3d.io.write_triangle_mesh(mesh_filename, mesh)\n",
    "    \n",
    "    o3d.io.write_point_cloud(pcd_filename, pcd_downsampled)\n",
    "\n",
    "    CSV_PATH = '/home/vigir3d/Datasets/cattle_scans/cow_measurements.csv'\n",
    "    # Check if CSV file exists to decide whether to write headers\n",
    "    write_header = not os.path.exists(CSV_PATH)\n",
    "  # Save SA & V to the CSV file in append mode\n",
    "    with open(CSV_PATH, 'a', newline='') as csvfile:\n",
    "        fieldnames = ['Animal ID', 'SA', 'V']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        if write_header:\n",
    "            writer.writeheader()\n",
    "\n",
    "        writer.writerow({'Animal ID': animal_id, 'SA': sa, 'V': v})\n",
    "\n",
    "        \n",
    "@jit(nopython=True, parallel=True)\n",
    "def numba_eliminate_flying_pixels(depth_image, ws, threshold):\n",
    "    height, width = depth_image.shape\n",
    "    result = np.zeros_like(depth_image, dtype=np.float64)\n",
    "    \n",
    "    for cy in prange(height):\n",
    "        for cx in prange(width):\n",
    "            x_start, x_end = max(0, cx - ws), min(width, cx + ws + 1)\n",
    "            y_start, y_end = max(0, cy - ws), min(height, cy + ws + 1)\n",
    "            window = depth_image[y_start:y_end, x_start:x_end]\n",
    "            result[cy, cx] = np.sum(np.abs(window - depth_image[cy, cx]))\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def load_maskrcnn_model(model_path):\n",
    "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\n",
    "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
    "    hidden_layer = 256\n",
    "    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask, hidden_layer, 2)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.load_state_dict(torch.load(model_path))\n",
    "    model.eval()\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_model_mask(model_generic, image):\n",
    "    proba_threshold = 0.5\n",
    "    ig = transform(image)\n",
    "    with torch.no_grad():\n",
    "        prediction = model_generic([ig.to(device)])\n",
    "        \n",
    "    if(prediction[0]['masks'].nelement() == 0):\n",
    "        XX = torch.empty((0,0), dtype=torch.int64)\n",
    "        return XX\n",
    "    predicted_mask = prediction[0]\n",
    "    predicted_mask = predicted_mask['masks'][0] > proba_threshold\n",
    "    \n",
    "    predicted_mask = predicted_mask.squeeze(1)\n",
    "    mask = predicted_mask.cpu().detach()\n",
    "    return mask\n",
    "\n",
    "def draw_registration_result(source, target, transformation):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.paint_uniform_color([1, 0.706, 0])\n",
    "    target_temp.paint_uniform_color([0, 0.651, 0.929])\n",
    "    source_temp.transform(transformation)\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp])\n",
    "    \n",
    "def segment_images_modified(last_frame):\n",
    "    depth_image_array = np.asarray(last_frame.depth)\n",
    "    print(\"Pre:\",np.max(depth_image_array), \" -- \", np.min(depth_image_array))\n",
    "    depth_PIL = Image.fromarray(np.asarray(last_frame.depth)).convert(\"RGB\")\n",
    "    rgb_image_array = np.asarray(last_frame.color)\n",
    "    rgb_PIL = Image.fromarray(rgb_image_array)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    rgb_mask = get_model_mask(model_rgb, rgb_PIL)\n",
    "    depth_mask = get_model_mask(model_depth, depth_PIL)\n",
    "\n",
    "    if depth_mask.nelement() == 0:\n",
    "        mask_combined = rgb_mask\n",
    "    else:\n",
    "        mask_combined = depth_mask | rgb_mask  # 1 vote arbitration (OR the masks)\n",
    "\n",
    "    # Convert tensor to numpy array and ensure the right datatype\n",
    "    mask_combined = mask_combined.numpy().astype(rgb_image_array.dtype)\n",
    "    mask_image = mask_combined.swapaxes(0, 2).swapaxes(0, 1)\n",
    "    mask_image = (mask_image > 0).astype(rgb_image_array.dtype)   \n",
    "\n",
    "    fg_image_rgb = rgb_image_array * mask_image\n",
    "\n",
    "    # For the depth image:\n",
    "    squeezed_mask = np.squeeze(mask_image)\n",
    "    fg_image_depth = (depth_image_array * squeezed_mask)*1000 # upscale because re-inserting in \"last_frame\" rescales\n",
    "    \n",
    "    print(\"Post:\" ,np.max(fg_image_depth), \" -- \", np.min(fg_image_depth))\n",
    "    last_frame.color = o3d.geometry.Image(fg_image_rgb)\n",
    "    last_frame.depth = o3d.geometry.Image(fg_image_depth)\n",
    "\n",
    "    return last_frame\n",
    "\n",
    "\n",
    "def cluster_point_cloud_new(outlier_cloud):\n",
    "    cloud_colors = copy.deepcopy(np.asarray(outlier_cloud.colors).T)\n",
    "    \n",
    "    with o3d.utility.VerbosityContextManager(o3d.utility.VerbosityLevel.Error) as cm:\n",
    "        labels = np.array(outlier_cloud.cluster_dbscan(eps=0.04, min_points=10, print_progress=False))\n",
    "\n",
    "    # Identify the largest cluster\n",
    "    values, counts = np.unique(labels, return_counts=True)\n",
    "    ind = np.argmax(counts)\n",
    "    largest_cluster_label = values[ind]\n",
    "    #print(f\"Largest cluster label: {largest_cluster_label}\")\n",
    "\n",
    "    # Filter points, normals, and colors for the largest cluster\n",
    "    cloud_xyz = pcd2xyz(outlier_cloud)\n",
    "    cloud_normals = pcd2normals(outlier_cloud)\n",
    "\n",
    "    cloud_filtered = cloud_xyz[:, labels == largest_cluster_label]\n",
    "    normals_filtered = cloud_normals[:, labels == largest_cluster_label]\n",
    "    colors_filtered = cloud_colors[:, labels == largest_cluster_label]\n",
    "\n",
    "    # Create a point cloud for the largest cluster\n",
    "    pcd_filtered_largest_cluster = o3d.geometry.PointCloud()\n",
    "    pcd_filtered_largest_cluster.points = o3d.utility.Vector3dVector(cloud_filtered.T)\n",
    "    pcd_filtered_largest_cluster.normals = o3d.utility.Vector3dVector(normals_filtered.T)\n",
    "    pcd_filtered_largest_cluster.colors = o3d.utility.Vector3dVector(colors_filtered.T)\n",
    "\n",
    "    #o3d.visualization.draw_geometries([pcd_filtered_largest_cluster])\n",
    "    return pcd_filtered_largest_cluster\n",
    "\n",
    "def load_calibration(tform_path, num_transforms=5):\n",
    "    transforms = [np.eye(4)]  # Initialize with identity matrix\n",
    "    \n",
    "    # Load transformations from file\n",
    "    for i in range(1, num_transforms+1):\n",
    "        filename = tform_path + f\"H_0_{i}.txt\"\n",
    "        \n",
    "        if not os.path.exists(filename):\n",
    "            raise FileNotFoundError(f\"The file {filename} does not exist!\")\n",
    "        \n",
    "        transforms.append(np.loadtxt(filename))\n",
    "    \n",
    "    return transforms\n",
    "\n",
    "def upsample_using_reference_normals_new(sparse_pcd, dense_pcd, search_radius=0.02, angle_threshold=30):\n",
    "    # Ensure the point clouds have normals\n",
    "    if not sparse_pcd.has_normals():\n",
    "        sparse_pcd.estimate_normals()\n",
    "    if not dense_pcd.has_normals():\n",
    "        dense_pcd.estimate_normals()\n",
    "    \n",
    "    # Ensure the point clouds have colors\n",
    "    if not sparse_pcd.has_colors():\n",
    "        sparse_pcd.paint_uniform_color([0.5, 0.5, 0.5])  # default gray color\n",
    "    if not dense_pcd.has_colors():\n",
    "        dense_pcd.paint_uniform_color([0.5, 0.5, 0.5])  # default gray color\n",
    "    \n",
    "    dense_points = np.asarray(dense_pcd.points)\n",
    "    dense_tree = cKDTree(dense_points)\n",
    "    \n",
    "    sparse_points = np.asarray(sparse_pcd.points)\n",
    "    sparse_normals = np.asarray(sparse_pcd.normals)\n",
    "    sparse_colors = np.asarray(sparse_pcd.colors)\n",
    "    \n",
    "    # This retrieves the indices of all neighbors within the search_radius\n",
    "    neighbor_indices = dense_tree.query_ball_point(sparse_points, search_radius, workers=-1)\n",
    "    \n",
    "    valid_indices = []\n",
    "    for i, idx_row in enumerate(neighbor_indices):\n",
    "        neighbors = dense_points[idx_row]\n",
    "        neighbor_normals = np.asarray(dense_pcd.normals)[idx_row]\n",
    "        neighbor_colors = np.asarray(dense_pcd.colors)[idx_row]\n",
    "        \n",
    "        # Calculate angles between sparse point's normal and all its neighbors' normals\n",
    "        angles = np.degrees(np.arccos(np.clip(np.dot(sparse_normals[i], neighbor_normals.T), -1.0, 1.0)))\n",
    "        \n",
    "        # Filtering neighbors based on angle threshold\n",
    "        valid_neighbor_indices = np.array(idx_row)[angles < angle_threshold]\n",
    "        valid_indices.extend(valid_neighbor_indices)\n",
    "    \n",
    "    unique_valid_indices = np.unique(valid_indices)\n",
    "    final_valid_neighbors = dense_points[unique_valid_indices]\n",
    "    final_valid_normals = np.asarray(dense_pcd.normals)[unique_valid_indices]\n",
    "    final_valid_colors = np.asarray(dense_pcd.colors)[unique_valid_indices]\n",
    "    \n",
    "    upsampled_points = np.vstack([sparse_points, final_valid_neighbors])\n",
    "    upsampled_normals = np.vstack([sparse_normals, final_valid_normals])\n",
    "    upsampled_colors = np.vstack([sparse_colors, final_valid_colors])\n",
    "    \n",
    "    upsampled_pcd = o3d.geometry.PointCloud()\n",
    "    upsampled_pcd.points = o3d.utility.Vector3dVector(upsampled_points)\n",
    "    upsampled_pcd.normals = o3d.utility.Vector3dVector(upsampled_normals)\n",
    "    upsampled_pcd.colors = o3d.utility.Vector3dVector(upsampled_colors)\n",
    "    \n",
    "    return upsampled_pcd\n",
    "\n",
    "\n",
    "def process_file_list_comp(file_id, dpath):\n",
    "    ws = 2\n",
    "    # Format filenames\n",
    "    dpath = dpath.rstrip('/')\n",
    "\n",
    "    # Split by the directory separator and get the last part\n",
    "    suffix = os.path.basename(dpath)\n",
    "    \n",
    "    file_suffix = suffix + \"_nano_\" + str(file_id)\n",
    "    # Construct full file paths using os.path.join for better cross-platform compatibility\n",
    "    depth_file = os.path.join(dpath, file_suffix + \"_depth.png\")\n",
    "    color_file = os.path.join(dpath, file_suffix + \".jpg\")\n",
    "    intrinsics_file = os.path.join(dpath, file_suffix + \"_intrinsics.txt\")\n",
    "    # Read the files\n",
    "    depth = o3d.io.read_image(depth_file)\n",
    "    color = o3d.io.read_image(color_file)\n",
    "    #depth = o3d.io.read_image(dpath + 'Animal_box3_u_nano_11_depth.png')\n",
    "    #color = o3d.io.read_image(dpath + 'Animal_box3_u_nano_11.jpg')\n",
    "    K = np.loadtxt(intrinsics_file)\n",
    "      \n",
    "\n",
    "    camera_intrinsics = o3d.camera.PinholeCameraIntrinsic()\n",
    "    camera_intrinsics.set_intrinsics(int(K[0]), int(K[1]), K[2], K[3], K[4], K[5])\n",
    "\n",
    "    rgbdc = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "        color, depth, depth_trunc=4.0, convert_rgb_to_intensity=False\n",
    "    )\n",
    "\n",
    "    volume = o3d.pipelines.integration.ScalableTSDFVolume(\n",
    "        voxel_length=4.0 / 512.0, sdf_trunc=0.04, color_type=o3d.pipelines.integration.TSDFVolumeColorType.RGB8\n",
    "    )\n",
    "\n",
    "    volume.integrate(rgbdc, camera_intrinsics, np.eye(4))\n",
    "    pcd_tsdf = volume.extract_point_cloud()\n",
    "    \n",
    "    depth_np = np.asarray(rgbdc.depth)\n",
    "    \n",
    "    if ( depth_np.max() > 20 ) : # Depth is in millimeters\n",
    "        flying_pixel_filter_threshold = 100 # 100\n",
    "    else : # meters\n",
    "        flying_pixel_filter_threshold = 0.1 # 0.1\n",
    "        #print(flying_pixel_filter_threshold, \" -- \", depth_np.max())\n",
    "\n",
    "    result_mask = numba_eliminate_flying_pixels(depth_np.copy(), ws, flying_pixel_filter_threshold)\n",
    "    depth_np[result_mask > flying_pixel_filter_threshold] = 0\n",
    "    # Re-insert filtered depth into the rgbd image\n",
    "    rgbdc.depth = o3d.geometry.Image(depth_np)\n",
    "    # Apply maskrcnn to filtered depth image\n",
    "    masked_rgbd = segment_images_modified(rgbdc)\n",
    "    # necessary lol\n",
    "    rgbdc_new = o3d.geometry.RGBDImage.create_from_color_and_depth(\n",
    "                    masked_rgbd.color, masked_rgbd.depth, depth_trunc=4.0, convert_rgb_to_intensity=False)\n",
    "    d = np.asarray(rgbdc_new.depth)\n",
    "    print(\"Segmented depth | max : \", np.max(d), \" min : \", np.min(d))\n",
    "\n",
    "    v = o3d.pipelines.integration.ScalableTSDFVolume(\n",
    "        voxel_length=4.0 / 512.0, sdf_trunc=0.04, color_type=o3d.pipelines.integration.TSDFVolumeColorType.RGB8\n",
    "    )\n",
    "\n",
    "    v.integrate(rgbdc_new, camera_intrinsics, np.eye(4))\n",
    "    pcd_tmp = v.extract_point_cloud()\n",
    "    #pcd_tmp = o3d.geometry.PointCloud.create_from_rgbd_image(rgbdc_new,\n",
    "     #                                                   camera_intrinsics)\n",
    "\n",
    "    return pcd_tmp, pcd_tsdf\n",
    "\n",
    "    \n",
    "def perform_pairwise_alignment(pcds_tsdf,pcds_cropped):\n",
    "    \"\"\"Compute and apply transformations.\"\"\"\n",
    "    t01 = colored_ICP(pcds_tsdf[0], pcds_tsdf[1])\n",
    "    t52 = colored_ICP(pcds_tsdf[5], pcds_tsdf[2])\n",
    "    t43 = colored_ICP(pcds_tsdf[4], pcds_tsdf[3])\n",
    "    t12 = colored_ICP(pcds_tsdf[1], pcds_tsdf[2])\n",
    "    t32 = colored_ICP(pcds_tsdf[3], pcds_tsdf[2])\n",
    "\n",
    "    H0 = t12 @ t01\n",
    "    H1 = t12\n",
    "    H3 = t32\n",
    "    H4 = t32 @ t43\n",
    "    H5 = t52\n",
    "\n",
    "    # Transform the point clouds\n",
    "    p0 = copy.deepcopy(pcds_cropped[0]).transform(H0)\n",
    "    p1 = copy.deepcopy(pcds_cropped[1]).transform(H1)\n",
    "    p3 = copy.deepcopy(pcds_cropped[3]).transform(H3)\n",
    "    p4 = copy.deepcopy(pcds_cropped[4]).transform(H4)\n",
    "    p5 = copy.deepcopy(pcds_cropped[5]).transform(H5)\n",
    "    \n",
    "    d0 = copy.deepcopy(pcds_tsdf[0]).transform(H0)\n",
    "    d1 = copy.deepcopy(pcds_tsdf[1]).transform(H1)\n",
    "    d3 = copy.deepcopy(pcds_tsdf[3]).transform(H3)\n",
    "    d4 = copy.deepcopy(pcds_tsdf[4]).transform(H4)\n",
    "    d5 = copy.deepcopy(pcds_tsdf[5]).transform(H5)\n",
    "    \n",
    "    pcd_combined = o3d.geometry.PointCloud()\n",
    "    pcd_combined = p0+p1+pcds_cropped[2]+p3+p4+p5\n",
    "    \n",
    "    ptsdf_combined = o3d.geometry.PointCloud()\n",
    "    ptsdf_combined = d0+d1+pcds_tsdf[2]+d3+d4+d5\n",
    "\n",
    "    return pcd_combined, ptsdf_combined\n",
    "\n",
    "    return  ptsdf_combined\n",
    "def pcd2normals(pcd):\n",
    "    return np.asarray(pcd.normals).T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4538432a",
   "metadata": {},
   "outputs": [],
   "source": [
    "0.12*1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38916c53",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "/home/vigir3d/Datasets/cattle_scans/Cattle_11_17_22/Animal_14_1/Animal_14_1_nano_11_intrinsics.txt not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Generate a list of volumes using list comprehension\u001b[39;00m\n\u001b[1;32m      6\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 8\u001b[0m results \u001b[38;5;241m=\u001b[39m [process_file_list_comp(file_id, path) \u001b[38;5;28;01mfor\u001b[39;00m file_id \u001b[38;5;129;01min\u001b[39;00m file_ids]\n\u001b[1;32m      9\u001b[0m pcds,ptsdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)\n\u001b[1;32m     10\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[28], line 8\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Generate a list of volumes using list comprehension\u001b[39;00m\n\u001b[1;32m      6\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 8\u001b[0m results \u001b[38;5;241m=\u001b[39m [\u001b[43mprocess_file_list_comp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m file_id \u001b[38;5;129;01min\u001b[39;00m file_ids]\n\u001b[1;32m      9\u001b[0m pcds,ptsdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)\n\u001b[1;32m     10\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[1], line 369\u001b[0m, in \u001b[0;36mprocess_file_list_comp\u001b[0;34m(file_id, dpath)\u001b[0m\n\u001b[1;32m    366\u001b[0m color \u001b[38;5;241m=\u001b[39m o3d\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mread_image(color_file)\n\u001b[1;32m    367\u001b[0m \u001b[38;5;66;03m#depth = o3d.io.read_image(dpath + 'Animal_box3_u_nano_11_depth.png')\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;66;03m#color = o3d.io.read_image(dpath + 'Animal_box3_u_nano_11.jpg')\u001b[39;00m\n\u001b[0;32m--> 369\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloadtxt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintrinsics_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    372\u001b[0m camera_intrinsics \u001b[38;5;241m=\u001b[39m o3d\u001b[38;5;241m.\u001b[39mcamera\u001b[38;5;241m.\u001b[39mPinholeCameraIntrinsic()\n\u001b[1;32m    373\u001b[0m camera_intrinsics\u001b[38;5;241m.\u001b[39mset_intrinsics(\u001b[38;5;28mint\u001b[39m(K[\u001b[38;5;241m0\u001b[39m]), \u001b[38;5;28mint\u001b[39m(K[\u001b[38;5;241m1\u001b[39m]), K[\u001b[38;5;241m2\u001b[39m], K[\u001b[38;5;241m3\u001b[39m], K[\u001b[38;5;241m4\u001b[39m], K[\u001b[38;5;241m5\u001b[39m])\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/numpy/lib/npyio.py:1338\u001b[0m, in \u001b[0;36mloadtxt\u001b[0;34m(fname, dtype, comments, delimiter, converters, skiprows, usecols, unpack, ndmin, encoding, max_rows, quotechar, like)\u001b[0m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(delimiter, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1336\u001b[0m     delimiter \u001b[38;5;241m=\u001b[39m delimiter\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1338\u001b[0m arr \u001b[38;5;241m=\u001b[39m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdelimiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelimiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1339\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconverters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskiplines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskiprows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musecols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musecols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1340\u001b[0m \u001b[43m            \u001b[49m\u001b[43munpack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munpack\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndmin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mndmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1341\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmax_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquote\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquotechar\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/numpy/lib/npyio.py:975\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(fname, delimiter, comment, quote, imaginary_unit, usecols, skiplines, max_rows, converters, ndmin, unpack, dtype, encoding)\u001b[0m\n\u001b[1;32m    973\u001b[0m     fname \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fname)\n\u001b[1;32m    974\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(fname, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 975\u001b[0m     fh \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_datasource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    976\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m encoding \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    977\u001b[0m         encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(fh, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatin1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/numpy/lib/_datasource.py:193\u001b[0m, in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;03mOpen `path` with `mode` and return the file object.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    189\u001b[0m \n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    192\u001b[0m ds \u001b[38;5;241m=\u001b[39m DataSource(destpath)\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnewline\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/numpy/lib/_datasource.py:533\u001b[0m, in \u001b[0;36mDataSource.open\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _file_openers[ext](found, mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[1;32m    531\u001b[0m                               encoding\u001b[38;5;241m=\u001b[39mencoding, newline\u001b[38;5;241m=\u001b[39mnewline)\n\u001b[1;32m    532\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 533\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: /home/vigir3d/Datasets/cattle_scans/Cattle_11_17_22/Animal_14_1/Animal_14_1_nano_11_intrinsics.txt not found."
     ]
    }
   ],
   "source": [
    "path = '/home/vigir3d/Datasets/cattle_scans/Cattle_11_17_22/Animal_14_1/'\n",
    "dpath = '/home/vigir3d/Datasets/cattle_scans/Cattle_11_17_22/Animal_1_1/'\n",
    "file_ids = [11,12,14,16,17,18]  # exclude head cameras\n",
    "\n",
    "# Generate a list of volumes using list comprehension\n",
    "start = time.time()\n",
    "\n",
    "results = [process_file_list_comp(file_id, path) for file_id in file_ids]\n",
    "pcds,ptsdf = zip(*results)\n",
    "end = time.time()\n",
    "print(\"Load generate: \" , end-start, \"s\")\n",
    "\n",
    "\n",
    "#r01 = np.loadtxt(dpath+\"r01.txt\" )\n",
    "#r12 = np.loadtxt(dpath+\"r12_0031.txt\")\n",
    "#r32 = np.loadtxt(dpath+\"r32_0026.txt\")\n",
    "#r52 = np.loadtxt(dpath+\"r52_005.txt\")\n",
    "#r43 = np.loadtxt(dpath+\"r43_0031.txt\")\n",
    "\n",
    "h0 = r12@r01\n",
    "h1 = r12\n",
    "h3 = r32\n",
    "h4 = r32@r43\n",
    "h5 = r52\n",
    "\n",
    "\n",
    "a0 = copy.deepcopy(pcds[0]).transform(h0)\n",
    "a1 = copy.deepcopy(pcds[1]).transform(h1)\n",
    "a3 = copy.deepcopy(pcds[3]).transform(h3)\n",
    "a4 = copy.deepcopy(pcds[4]).transform(h4)\n",
    "a5 = copy.deepcopy(pcds[5]).transform(h5)\n",
    "\n",
    "o3d.visualization.draw_geometries([a0,a1,a3,a4,a5,pcds[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b271b77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a0 = copy.deepcopy(pcds[0]).transform(h0)\n",
    "a1 = copy.deepcopy(pcds[1]).transform(h1)\n",
    "a3 = copy.deepcopy(pcds[3]).transform(h3)\n",
    "a4 = copy.deepcopy(pcds[4]).transform(h4)\n",
    "a5 = copy.deepcopy(pcds[5]).transform(h5)\n",
    "\n",
    "o3d.visualization.draw_geometries([a0,a1,a3,a4,a5,pcds[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24ba8275",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcd_f = o3d.geometry.PointCloud()\n",
    "pcd_f = a0+a1+a3+a4+a5+pcds[2]\n",
    "#o3d.io.write_point_cloud(dpath+\"animal_1_1_ds.ply\", pcd_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8cbd03b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = 0.05\n",
    "a = cluster_point_cloud_new(pcd_f,dist)\n",
    "draw_registration_result(pcd_f,a,np.eye(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd79da06",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a4de182f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#p = upsample_using_reference_normals_new(a,pcd_a,0.02)\n",
    "c = upsample_using_reference_normals_adaptable(a,pcd_a,0.02,0.1,30,100)\n",
    "#draw_registration_result(p,a,np.eye(4))\n",
    "draw_registration_result(c,a,np.eye(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f63c4ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def upsample_using_reference_normals_adaptable(sparse_pcd, dense_pcd, initial_search_radius=0.02, max_search_radius=0.1, angle_threshold=30, density_threshold=10):\n",
    "    if not sparse_pcd.has_normals():\n",
    "        sparse_pcd.estimate_normals()\n",
    "    if not dense_pcd.has_normals():\n",
    "        dense_pcd.estimate_normals()\n",
    "\n",
    "    if not sparse_pcd.has_colors():\n",
    "        sparse_pcd.paint_uniform_color([0.5, 0.5, 0.5])\n",
    "    if not dense_pcd.has_colors():\n",
    "        dense_pcd.paint_uniform_color([0.5, 0.5, 0.5])\n",
    "    \n",
    "    dense_points = np.asarray(dense_pcd.points)\n",
    "    dense_tree = cKDTree(dense_points)\n",
    "    \n",
    "    sparse_points = np.asarray(sparse_pcd.points)\n",
    "    sparse_normals = np.asarray(sparse_pcd.normals)\n",
    "    sparse_colors = np.asarray(sparse_pcd.colors)\n",
    "\n",
    "    valid_indices = []\n",
    "    for i, sparse_point in enumerate(sparse_points):\n",
    "        search_radius = initial_search_radius\n",
    "        while True:\n",
    "            neighbor_indices = dense_tree.query_ball_point(sparse_point, search_radius)\n",
    "            if len(neighbor_indices) >= density_threshold or search_radius >= max_search_radius:\n",
    "                break  # Sufficient density or max radius reached\n",
    "            search_radius += 0.01  # Increment search radius\n",
    "\n",
    "        neighbors = dense_points[neighbor_indices]\n",
    "        neighbor_normals = np.asarray(dense_pcd.normals)[neighbor_indices]\n",
    "        neighbor_colors = np.asarray(dense_pcd.colors)[neighbor_indices]\n",
    "\n",
    "        angles = np.degrees(np.arccos(np.clip(np.dot(sparse_normals[i], neighbor_normals.T), -1.0, 1.0)))\n",
    "\n",
    "        valid_neighbor_indices = np.array(neighbor_indices)[angles < angle_threshold]\n",
    "        valid_indices.extend(valid_neighbor_indices)\n",
    "    \n",
    "    unique_valid_indices = np.unique(valid_indices)\n",
    "    final_valid_neighbors = dense_points[unique_valid_indices]\n",
    "    final_valid_normals = np.asarray(dense_pcd.normals)[unique_valid_indices]\n",
    "    final_valid_colors = np.asarray(dense_pcd.colors)[unique_valid_indices]\n",
    "\n",
    "    upsampled_points = np.vstack([sparse_points, final_valid_neighbors])\n",
    "    upsampled_normals = np.vstack([sparse_normals, final_valid_normals])\n",
    "    upsampled_colors = np.vstack([sparse_colors, final_valid_colors])\n",
    "\n",
    "    upsampled_pcd = o3d.geometry.PointCloud()\n",
    "    upsampled_pcd.points = o3d.utility.Vector3dVector(upsampled_points)\n",
    "    upsampled_pcd.normals = o3d.utility.Vector3dVector(upsampled_normals)\n",
    "    upsampled_pcd.colors = o3d.utility.Vector3dVector(upsampled_colors)\n",
    "\n",
    "    return upsampled_pcd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d850f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsample_using_reference_normals_new(sparse_pcd, dense_pcd, search_radius=0.02, angle_threshold=30):\n",
    "    # Ensure the point clouds have normals\n",
    "    if not sparse_pcd.has_normals():\n",
    "        sparse_pcd.estimate_normals()\n",
    "    if not dense_pcd.has_normals():\n",
    "        dense_pcd.estimate_normals()\n",
    "    \n",
    "    # Ensure the point clouds have colors\n",
    "    if not sparse_pcd.has_colors():\n",
    "        sparse_pcd.paint_uniform_color([0.5, 0.5, 0.5])  # default gray color\n",
    "    if not dense_pcd.has_colors():\n",
    "        dense_pcd.paint_uniform_color([0.5, 0.5, 0.5])  # default gray color\n",
    "    \n",
    "    dense_points = np.asarray(dense_pcd.points)\n",
    "    dense_tree = cKDTree(dense_points)\n",
    "    \n",
    "    sparse_points = np.asarray(sparse_pcd.points)\n",
    "    sparse_normals = np.asarray(sparse_pcd.normals)\n",
    "    sparse_colors = np.asarray(sparse_pcd.colors)\n",
    "    \n",
    "    # This retrieves the indices of all neighbors within the search_radius\n",
    "    neighbor_indices = dense_tree.query_ball_point(sparse_points, search_radius, workers=-1)\n",
    "    \n",
    "    valid_indices = []\n",
    "    for i, idx_row in enumerate(neighbor_indices):\n",
    "        neighbors = dense_points[idx_row]\n",
    "        neighbor_normals = np.asarray(dense_pcd.normals)[idx_row]\n",
    "        neighbor_colors = np.asarray(dense_pcd.colors)[idx_row]\n",
    "        \n",
    "        # Calculate angles between sparse point's normal and all its neighbors' normals\n",
    "        angles = np.degrees(np.arccos(np.clip(np.dot(sparse_normals[i], neighbor_normals.T), -1.0, 1.0)))\n",
    "        \n",
    "        # Filtering neighbors based on angle threshold\n",
    "        valid_neighbor_indices = np.array(idx_row)[angles < angle_threshold]\n",
    "        valid_indices.extend(valid_neighbor_indices)\n",
    "    \n",
    "    unique_valid_indices = np.unique(valid_indices)\n",
    "    final_valid_neighbors = dense_points[unique_valid_indices]\n",
    "    final_valid_normals = np.asarray(dense_pcd.normals)[unique_valid_indices]\n",
    "    final_valid_colors = np.asarray(dense_pcd.colors)[unique_valid_indices]\n",
    "    \n",
    "    upsampled_points = np.vstack([sparse_points, final_valid_neighbors])\n",
    "    upsampled_normals = np.vstack([sparse_normals, final_valid_normals])\n",
    "    upsampled_colors = np.vstack([sparse_colors, final_valid_colors])\n",
    "    \n",
    "    upsampled_pcd = o3d.geometry.PointCloud()\n",
    "    upsampled_pcd.points = o3d.utility.Vector3dVector(upsampled_points)\n",
    "    upsampled_pcd.normals = o3d.utility.Vector3dVector(upsampled_normals)\n",
    "    upsampled_pcd.colors = o3d.utility.Vector3dVector(upsampled_colors)\n",
    "    \n",
    "    return upsampled_pcd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39612c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_point_cloud_new(outlier_cloud,dist):\n",
    "    cloud_colors = copy.deepcopy(np.asarray(outlier_cloud.colors).T)\n",
    "    \n",
    "    with o3d.utility.VerbosityContextManager(o3d.utility.VerbosityLevel.Error) as cm:\n",
    "        labels = np.array(outlier_cloud.cluster_dbscan(eps=dist, min_points=10, print_progress=False))\n",
    "\n",
    "    # Identify the largest cluster\n",
    "    values, counts = np.unique(labels, return_counts=True)\n",
    "    ind = np.argmax(counts)\n",
    "    largest_cluster_label = values[ind]\n",
    "    #print(f\"Largest cluster label: {largest_cluster_label}\")\n",
    "\n",
    "    # Filter points, normals, and colors for the largest cluster\n",
    "    cloud_xyz = pcd2xyz(outlier_cloud)\n",
    "    cloud_normals = pcd2normals(outlier_cloud)\n",
    "\n",
    "    cloud_filtered = cloud_xyz[:, labels == largest_cluster_label]\n",
    "    normals_filtered = cloud_normals[:, labels == largest_cluster_label]\n",
    "    colors_filtered = cloud_colors[:, labels == largest_cluster_label]\n",
    "\n",
    "    # Create a point cloud for the largest cluster\n",
    "    pcd_filtered_largest_cluster = o3d.geometry.PointCloud()\n",
    "    pcd_filtered_largest_cluster.points = o3d.utility.Vector3dVector(cloud_filtered.T)\n",
    "    pcd_filtered_largest_cluster.normals = o3d.utility.Vector3dVector(normals_filtered.T)\n",
    "    pcd_filtered_largest_cluster.colors = o3d.utility.Vector3dVector(colors_filtered.T)\n",
    "\n",
    "    #o3d.visualization.draw_geometries([pcd_filtered_largest_cluster])\n",
    "    return pcd_filtered_largest_cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67aa048",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpath+\"animal_1_1_ds.ply\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1cc2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pcds)):\n",
    "    o3d.visualization.draw_geometries([pcds[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d216442f",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([ptsdf[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "757330bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rgb_model_path = '/home/vigir3d/Datasets/cattle_scans/maskrcnn_data/maskrcnn_v2.pth'\n",
    "depth_model_path = '/home/vigir3d/Datasets/cattle_scans/maskrcnn_data/maskrcnn_depth_best.pth'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Loads the maskrcnn trained for depth and rgb\n",
    "model_rgb = load_maskrcnn_model(rgb_model_path)\n",
    "model_depth = load_maskrcnn_model(depth_model_path)\n",
    "\n",
    "\n",
    "transform = T.ToTensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d75a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd17574",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15f39536",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dpath' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m r01 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(\u001b[43mdpath\u001b[49m\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr01.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n\u001b[1;32m      2\u001b[0m r12 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(dpath\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr12_0031.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m r32 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mloadtxt(dpath\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr32_0026.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dpath' is not defined"
     ]
    }
   ],
   "source": [
    "r01 = np.loadtxt(dpath+\"r01.txt\" )\n",
    "r12 = np.loadtxt(dpath+\"r12_0031.txt\")\n",
    "r32 = np.loadtxt(dpath+\"r32_0026.txt\")\n",
    "r52 = np.loadtxt(dpath+\"r52_005.txt\")\n",
    "r43 = np.loadtxt(dpath+\"r43_0031.txt\")\n",
    "\n",
    "h0 = r12@r01\n",
    "h1 = r12\n",
    "h3 = r32\n",
    "h4 = r32@r43\n",
    "h5 = r52\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7e0699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # transform!\n",
    "# np.savetxt(dpath+\"rq01.txt\", r01)\n",
    "# np.savetxt(dpath+\"rq12_0031.txt\", r12)\n",
    "# np.savetxt(dpath+\"rq32_0026.txt\", r32)\n",
    "# np.savetxt(dpath+\"rq52_005.txt\", r52)\n",
    "# np.savetxt(dpath+\"rq43_0031.txt\", r43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47b0b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pcds)):\n",
    "    o3d.visualization.draw_geometries([pcds[i],ptsdf[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a926180",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([ptsdf[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbdaae32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms\n",
    "h0 = r12@r01\n",
    "h1 = r12\n",
    "h3 = r32\n",
    "h4 = r32@r43\n",
    "h5 = r52\n",
    "\n",
    "p0 = copy.deepcopy(ptsdf[0]).transform(h0)\n",
    "p1 = copy.deepcopy(ptsdf[1]).transform(h1)\n",
    "p3 = copy.deepcopy(ptsdf[3]).transform(h3)\n",
    "p4 = copy.deepcopy(ptsdf[4]).transform(h4)\n",
    "p5 = copy.deepcopy(ptsdf[5]).transform(h5)\n",
    "\n",
    "pcd_a = o3d.geometry.PointCloud()\n",
    "pcd_a = p0+p1+p3+p4+p5+ptsdf[2]\n",
    "#o3d.io.write_point_cloud(dpath+\"animal_1_1_ds.ply\", pcd_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5068666",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pcds)):\n",
    "    o3d.visualization.draw_geometries([pcds[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd76b1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([d0,d1,d3,d4,d5,pcds[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cdda4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502962bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886984e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cf0942",
   "metadata": {},
   "outputs": [],
   "source": [
    "o3d.visualization.draw_geometries([p0,p1,p3,p4,p5,ptsdf[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dec461f",
   "metadata": {},
   "outputs": [],
   "source": [
    "vs = 0.1\n",
    "#t01,r01 = register_two_views_teaser(ptsdf[0],ptsdf[1],vs)\n",
    "#t12,r12 = register_two_views_teaser(ptsdf[1],ptsdf[2],vs) # 0.031\n",
    "#t32,r32 = register_two_views_teaser(ptsdf[3],ptsdf[2],vs) # 0.026\n",
    "#t52,r52 = register_two_views_teaser(ptsdf[5],ptsdf[2],0.05)\n",
    "#t43,r43 = register_two_views_teaser(ptsdf[4],ptsdf[3],vs) # 0.031\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096f3c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r01 = demo_manual_registration(ptsdf[0],ptsdf[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06c22bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_registration_result(source, target, transformation):\n",
    "    source_temp = copy.deepcopy(source)\n",
    "    target_temp = copy.deepcopy(target)\n",
    "    source_temp.paint_uniform_color([1, 0.706, 0])\n",
    "    target_temp.paint_uniform_color([0, 0.651, 0.929])\n",
    "    source_temp.transform(transformation)\n",
    "    o3d.visualization.draw_geometries([source_temp, target_temp])\n",
    "\n",
    "\n",
    "def pick_points(pcd):\n",
    "    print(\"\")\n",
    "    print(\n",
    "        \"1) Please pick at least three correspondences using [shift + left click]\"\n",
    "    )\n",
    "    print(\"   Press [shift + right click] to undo point picking\")\n",
    "    print(\"2) After picking points, press 'Q' to close the window\")\n",
    "    vis = o3d.visualization.VisualizerWithEditing()\n",
    "    vis.create_window()\n",
    "    vis.add_geometry(pcd)\n",
    "    vis.run()  # user picks points\n",
    "    vis.destroy_window()\n",
    "    print(\"\")\n",
    "    return vis.get_picked_points()\n",
    "\n",
    "\n",
    "def demo_manual_registration(src, tgt):\n",
    "    print(\"Demo for manual ICP\")\n",
    "    source = src\n",
    "    target = tgt\n",
    "    print(\"Visualization of two point clouds before manual alignment\")\n",
    "    draw_registration_result(source, target, np.identity(4))\n",
    "\n",
    "    # pick points from two point clouds and builds correspondences\n",
    "    picked_id_source = pick_points(source)\n",
    "    picked_id_target = pick_points(target)\n",
    "    assert (len(picked_id_source) >= 3 and len(picked_id_target) >= 3)\n",
    "    assert (len(picked_id_source) == len(picked_id_target))\n",
    "    corr = np.zeros((len(picked_id_source), 2))\n",
    "    corr[:, 0] = picked_id_source\n",
    "    corr[:, 1] = picked_id_target\n",
    "\n",
    "    # estimate rough transformation using correspondences\n",
    "    print(\"Compute a rough transform using the correspondences given by user\")\n",
    "    p2p = o3d.pipelines.registration.TransformationEstimationPointToPoint()\n",
    "    trans_init = p2p.compute_transformation(source, target,\n",
    "                                            o3d.utility.Vector2iVector(corr))\n",
    "\n",
    "    # point-to-point ICP for refinement\n",
    "    print(\"Perform point-to-point ICP refinement\")\n",
    "    threshold = 0.03  # 3cm distance threshold\n",
    "    reg_p2p = o3d.pipelines.registration.registration_icp(\n",
    "        source, target, threshold, trans_init,\n",
    "        o3d.pipelines.registration.TransformationEstimationPointToPoint())\n",
    "    draw_registration_result(source, target, reg_p2p.transformation)\n",
    "    return reg_p2p.transformation\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2fc4ef6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
